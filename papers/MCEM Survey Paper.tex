\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry} 
\usepackage{color}               		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{setspace}

\usepackage[title]{appendix}   % Start an appendices environment, then treat each separate appendix as an ordinary section
								% [title] changes labels to, e.g., "Appendix A...". Omit to label as "A...".

\usepackage{authblk}

\usepackage{mathrsfs}	% For \mathcal{}, a script font in math mode

\usepackage[semicolon]{natbib}
\usepackage{verbatim}
\usepackage{soul}	% Highlighting
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}
\def\UrlBreaks{\do\/\do-}


\usepackage{multirow}

\usepackage{esdiff} %Shorter syntax for derivatives. Use \diff{f}{x} or \diffp{f}{t}
\usepackage{amsmath}
\usepackage{amsthm}

\usepackage{enumitem}   % For more customizable lists

\usepackage{etoolbox}   % Prerequisite for imakeidx
\usepackage{imakeidx}   % For making an index
\makeindex              % Initialize the index (this command must be included!)


% \usepackage[ruled]{algorithm2e} % For writing algorithms

\usepackage{algpseudocode}
\usepackage{algorithm} % For writing algorithms

\usepackage{glossaries}
\makeglossaries



\newcommand{\lt}{LTHC}
\newcommand{\llt}{\ell(\theta)}


\newcommand{\bF}{\mathbb{F}}
\newcommand{\bG}{\mathbb{G}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\bQ}{\mathbb{Q}}
\newcommand{\bV}{\mathbb{V}}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\bR}{\mathbb{R}}

\newcommand{\iid}{\overset{\mathrm{iid}}{\sim}}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\cd}{f_c(y, x; \theta)}
\newcommand{\od}{f(y; \theta)}

\newcommand{\hq}{\hat{Q}}

\newcommand{\wcirc}{\mathring{w}}

\newcommand{\mh}{Metropolis-Hastings}


\newtheorem{proposition}{Proposition}[section]



\newglossaryentry{densities} %! This doesn't actually work. Inline glossary entries are like citations, they print the name of the entry. I could instead separate each density into its own glossary entry, but this will lead to a large, poorly organized glossary.
{
    name={$f, f_c, f_m$},
    description={Observed data, complete data, missing data densities (or PMFs)}
}


\title{\SARS\ Transmission in University Classes}
\author[1]{William Ruth}
\author[2]{Richard Lockhart}
\affil[1]{Corresponding Author - Department of Statistics and Actuarial Science \\ Simon Fraser University \\ Burnaby, BC  Canada \\ wruth@sfu.ca}
\affil[2]{Department of Statistics and Actuarial Science \\ Simon Fraser University \\ Burnaby, BC  Canada}
%\affil{Department of Statistics and Actuarial Science \\ Simon Fraser University \\ Burnaby, BC  Canada \\ lockhart@sfu.ca}
%\date{\today}							% Activate to display a given date or no date
\date{}

\begin{document}
%\maketitle

% \doublespacing

\begin{abstract}
    We survey the EM algorithm and its Monte Carlo-based extensions.
\end{abstract}

\begin{itemize}
    \color{red}
    \item Replace ``conditional distribution of the missing data given the observed data'' with ``missing data distribution''
    \item \citet{Caf05} use $M_k$ for the Monte Carlo size at iteration $k$. This is a useful definition for discussing other papers too.
    \item I have changed my iteration labels. $\hat{\theta}_{k-1}$ is now the maximizer of the current MCEM objective function, and $\hat{\theta}_{k-1}$ was used to construct that objective function. Watch for this when editing.
    \item Parameter space has not yet been defined.
    \item Is the ``a'' in EM algorithm capitalized or not?
\end{itemize}

\section{The EM Algorithm}

%todo - Define observed, complete and missing data distributions
%todo - Define parameter space.

We define three distributions which will be central to our study of missing data problems. Let $Y$ be the observed data and $X$ be the missing data. Note that $X$ need not correspond to any actual real-world process, but may instead be a conceptual device which facilitates analysis of the data which were actually observed. We refer to the distribution of $Y$ as the ``observed data distribution''\index{Observed Data Distribution}, and write $f$ for its density (or mass function). We refer to the joint distribution of $Y$ and $X$ as the ``complete data distribution''\index{Complete Data Distribution}, and write $f_c$ for its density. We refer to the conditional distribution of $X$ given $Y$ as the ``missing data distribution''\index{Missing Data Distribution}, and write $f_m$ for its density. Note that the missing data distribution is not the marginal distribution of the missing data, but rather its conditional distribution given the observed data.

The EM algorithm is a method for analyzing incomplete data which was formalized by \citet{Dem77}. See \citet{McL08} for an excellent book-length overview of the EM algorithm. We begin by discussing a probabilistic framework within which the EM algorithm is often applied. We then present the EM algorithm in detail. Finally, we discuss some limitations of this method. Throughout, we illustrate our presentation with a toy problem based on linear regression with a single, unobserved, covariate.

The EM algorithm consists of iterating two steps. First is the expectation, or ``E'', step, in which an objective function is constructed from the complete data likelihood. Second is the maximization, or ``M'', step, in which the previously computed objective function is maximized. These two steps are then alternated until some convergence criterion is met. Whatever value of $\theta$ the algorithm converges to is used as our parameter estimate. We now go into more detail on each of the two steps.

The E-step of the EM algorithm is where we construct the objective function which will be used to update our parameter estimate. This objective function is the conditional expectation of the complete data likelihood, given the observed data. If our complete data can be partitioned into an observed component, $Y$, and a missing component, $X$, then our objective function at iteration $k$ is given by
%
\begin{align}
    Q(\theta|\theta_{k-1}) & = \bE_{\theta_{k-1}}[\ell_c(\theta; y, X) | Y=y]
\end{align}
%
where $\ell_c$ is the log-likelihood of the complete data model. Note that the conditional expectation uses our parameter estimate from the previous iteration.

The M-step of the EM algorithm consists of maximizing the objective function constructed in the previous E-step. That is, we define $\theta_k = \argmax\limits_\theta Q(\theta|\theta_{k-1})$. Typically, this optimization must be performed numerically via, e.g., gradient ascent or Newton's method. See \citet{Noc06} for details and other optimization algorithms. 

We can combine the E- and M-steps of the EM algorithm into a single ``update function''. We write $M(\theta_{k-1}) = \argmax\limits_\theta Q(\theta|\theta_{k-1})$. The EM algorithm can thus be viewed as the iterative application of this update function, $M$.

\subsection{Properties}

\textbf{Section intro...}

\subsubsection{Ascent Property and Generalized EM}
\label{sec:GEM}

An important feature of the EM algorithm is its so-called ``ascent property''. This property says that an iteration of the EM algorithm (\hl{have I explicitly defined ``EM iteration''? Do I need to?}) never decreases the observed data likelihood. This is somewhat surprising, since EM updates are computed without ever evaluating the observed data likelihood. 

\begin{proposition}[Ascent Property of EM]
    Let $\theta \in \Theta$, and $\theta' = M(\theta)$ be the EM update from $\theta$. Then $\ell(\theta') \geq \ell(\theta)$.
\end{proposition}

\begin{proof}
    We begin by noting that the following decomposition holds for any value of $x$:
    %
    \begin{align}
        \ell(\theta; y) &= \ell_c(\theta; y, x) - \ell_m(\theta; y, x)
    \end{align}
    %
    Subtracting the values of both sides at $\theta$ from their values at $\theta'$ and taking conditional expectations, we get
    %
    \begin{align}
        \ell(\theta'; y) - \ell(\theta; y) &= Q(\theta'|\theta) - Q(\theta|\theta) + \bE_{\theta}[\ell_m(\theta; y, x) - \ell_m(\theta'; y, x)]\\
        &= Q(\theta'|\theta) - Q(\theta|\theta) + \mathrm{KL}(\theta || \theta') \label{eq:asc_KL}
    \end{align}
    %
    where the last term in line \ref{eq:asc_KL} is the Kullback-Leibler (KL) divergence from the missing data distribution with $\theta = \theta$ to the same distribution with $\theta = \theta'$. Note that KL divergences are always non-negative, so we get
    %
    \begin{align}
    \ell(\theta'; y) - \ell(\theta; y) &\geq Q(\theta'|\theta) - Q(\theta|\theta)    
    \end{align}
    %
    Finally, since $\theta'$ maximizes $Q(\cdot|\theta)$, we have $\ell(\theta'; y) - \ell(\theta; y) \geq 0$.
\end{proof}

In our proof of the ascent property, we only required that $Q(\theta'|\theta) \geq Q(\theta|\theta)$, not that $\theta'$ maximize $Q(\cdot|\theta)$. This observation leads to the definition of the ``Generalized EM Algorithm''\index{Generalized EM Algorithm}, which replaces the M-step with setting $\theta_k$ to any point in $\Theta$ such that $Q(\theta_k|\theta_{k-1}) \geq Q(\theta_{k-1}|\theta_{k-1})$.



\subsubsection{Recovering Observed Data Likelihood Quantities}

Under regularity conditions, it is possible to compute both the score vector and the observed information matrix of the observed data likelihood using complete data quantities. These regularity conditions consist of being able to interchange the order of differentiation and integration for various functions (\hl{awk? I wasn't feeling well when I wrote this.}). \hl{Does it make sense to define $\mathcal{I}_c$ and $\mathcal{I}_m$ in the following proposition?}

\begin{proposition}
    \label{thm:EM_decomp}
    The following identities hold (\hl{regularity conditions!}):
    \begin{enumerate}[label=(\roman*)]
        \item $S(\theta; y) = \bE_\theta [S_c(\theta; y, X)|Y=y]$ \label{eq:obs_score_identity}
        \item $I(\theta) = \mathcal{I}_c(\theta) - \mathcal{I}_m(\theta)$ \label{eq:obs_info_identity}\\
        where $\mathcal{I}_c(\theta) := - \bE_\theta \left[ \nabla^2 \ell_c(\theta; y, X) | Y=y \right]$ and $\mathcal{I}_m(\theta) := - \bE_\theta \left[ \nabla^2 \ell_m(\theta; y, X) | Y=y \right]$
    \end{enumerate}
\end{proposition}

\begin{proof}
    We start with expression \ref{eq:obs_score_identity}. Let $\Omega$ be the complete data sample space. Let $\mathcal{Y}$ and $\mathcal{X}$ be the observed and missing data sample spaces respectively. For every $y \in \mathcal{Y}$, let $\mathcal{X}(y) = \{ x \in \mathcal{X}: (y,x) \in \Omega\}$. Note that $f(y; \theta) = \int_{\mathcal{X}(y)} f_c(y, x; \theta) dx$.
    %
    \begin{align}
        \bE_\theta [S_c(\theta; y, X)|Y=y] &= \int_{\mathcal{X}(y)} \nabla \ell_c(\theta; y, x) f_m(y, x; \theta) dx \nonumber\\
        &= \int_{\mathcal{X}(y)} \frac{f_m(y, x; \theta)}{f_c(y, x; \theta)} \nabla f_c(\theta; y, x) dx \nonumber\\
        &= \int_{\mathcal{X}(y)} \frac{1}{f(y; \theta)} \nabla f_c(\theta; y, x) dx\nonumber\\
        &= \frac{1}{f(y; \theta)} \int_{\mathcal{X}(y)} \nabla f_c(\theta; y, x) dx\nonumber\\
        &= \frac{1}{f(y; \theta)} \nabla \int_{\mathcal{X}(y)} f_c(\theta; y, x) dx \nonumber\\
        &= \frac{1}{f(y; \theta)} \nabla f(y; \theta)\nonumber\\
        &= S(\theta; y) \nonumber
    \end{align}

    Proceeding now to \ref{eq:obs_info_identity}, we decompose the observed data log-likelihood as
    %
    \begin{align*}
        \ell(\theta; y) &= \ell_c(\theta; y, x) - \ell_m(\theta; y, x)
    \end{align*}
    %
    Differentiating twice and taking conditional expectations of both sides yields the required result.
\end{proof}

An alternative to Proposition \ref{thm:EM_decomp} part \ref{eq:obs_info_identity} which involves only conditional expectations of complete data quantities is given in the following proposition.

\begin{proposition}
    \label{thm:info_decomp}
    Let $\hat{\theta}$ be a stationary point of the observed data log-likelihood. \hl{Assuming regularity conditions,} we can write the observed information of the observed data distribution at $\hat{\theta}$ as
    %
    \begin{align}
        I(\theta) = \mathcal{I}_c(\theta) - \bE_{\theta} [ S_c(\theta) S_c(\theta)^T | Y=y] + S(\theta) S(\theta)^T
    \end{align}
    %
    In particular, if $\hat{\theta}$ is a stationary point of the observed data log-likelihood, then
    %
    \begin{align}
        I(\hat{\theta}) = \mathcal{I}_c(\hat{\theta})  - \bE_{\hat{\theta}} [ S_c(\hat{\theta}) S_c(\hat{\theta})^T | Y=y]
    \end{align}
\end{proposition}

\begin{proof}
    We follow the derivation of \citet{Lou82}. For brevity, we write $f(\theta)$ and $f_c(\theta)$ for $f(y; \theta)$ and $f(y, x; \theta)$ respectively. Consider the following two Hessians:
    %
    \begin{align}
        \nabla^2 \ell(\theta) &= \nabla \left[ \int_{\mathcal{X}(y)} \frac{\nabla f_c(\theta) dx}{f(\theta)} \right]\\
        &= \int_{\mathcal{X}(y)} \frac{\nabla^2 f_c(\theta)}{f(\theta)} dx - \frac{1}{f(\theta)^2}\left( \int_{\mathcal{X}(y)} \nabla f_c(\theta) dx \right) \left( \int_{\mathcal{X}(y)} \nabla f_c(\theta) dx \right)^T\\
        &= \bE_\theta \left[ \left. \frac{\nabla^2 f_c(\theta)}{f_c(\theta)} \right| Y=y \right] - \bE_\theta \left[ \left. \frac{\nabla f_c(\theta)}{f_c(\theta)} \right| Y=y \right] \bE_\theta \left[ \left. \frac{\nabla f_c(\theta)}{f_c(\theta)} \right| Y=y \right]^T\\
        &= \bE_\theta \left[ \left. \frac{\nabla^2 f_c(\theta)}{f_c(\theta)} \right| Y=y \right] - S(\theta; y) S(\theta; y)^T \label{eq:hess_obs_lik}\\
        \nabla^2 \ell_c(\theta) &= \nabla \left( \frac{\nabla f_c(\theta)}{f_c(\theta)} \right)\\
        &= \frac{\nabla^2 f_c(\theta)}{f_c(\theta)} - S_c(\theta) S_c(\theta)^T \label{eq:hess_comp_lik}
    \end{align}
    %
    Combining lines \ref{eq:hess_obs_lik} and \ref{eq:hess_comp_lik}, we get
    %
    \begin{align}
        \nabla^2 \ell(\theta) &= \bE_\theta [ \nabla^2 \ell_c(\theta) | Y=y] + \bE_\theta [ S_c(\theta) S_c(\theta)^T | Y=y] - S(\theta; y) S(\theta; y)^T \label{eq:hess_obs_lik2}
    \end{align}
    %
    Finally, evaluating line \ref{eq:hess_obs_lik2} at $\theta = \hat{\theta}$ makes the rightmost term vanish, thereby yielding the required expression.
\end{proof}

Proposition \ref{thm:info_decomp} is known as Louis' standard error formula. Other decompositions for the observed information matrix of the observed data likelihood do exist; see, e.g., \citet{Oak99,McL08}. However, the one due to Louis will be most useful to us later.

\subsection{Example: Gene Frequency Estimation}
\label{sec:eg-genes}

Consider the problem of estimating allele frequencies based on observed phenotypes. Often, a single phenotype can be encoded by multiple genotypes with different configurations of dominant and recessive alleles. This is sometimes referred to as the problem of gene frequency estimation\index{Gene Frequency Estimation}. Our analysis closely follows Example 2.4 from \citet{McL08}\footnote{Although these authors do give data in their example, there is little context around this data, and I have been unable to locate more information from their references. I have opted instead to use my own dataset.}

We investigate a simplified model for blood type which consists of only the ABO blood group. See Chapter 5 of \citet{Dea05}, for a detailed introduction to blood types\index{Blood Type}. There are three alleles for this gene: A, B and O. Allele O is recessive, which alleles A and B exhibit co-dominance. That is, genotypes AO and AA encode blood type A, genotypes BO and BB encode blood type B, genotype OO encodes blood type O, and genotype AB encodes blood type AB. Suppose that we seek to estimate the proportion of each allele within a population, based on a sample of individuals' phenotypes. \citet{Fuj78} report blood types of 4,464,349 people in Japan collected between 1964 and 1975. This sample is so large that any standard errors are very small. To retain a reasonable level of uncertainty, we focus on a single administrative division, Oto, in Nara Prefecture. See Figure \ref{tab:blood_type} for details.

\begin{table}
    \centering
    \caption{Observed frequency and theoretical probability of each blood type \citep{Fuj78}}
    \begin{tabular}{c|cccc}
        Blood Type & O & A & B & AB\\
        \hline
        Random Variable & $Y_1$ & $Y_2$ & $Y_3$ & $Y_4$\\
        Observed Frequency & 10 & 16 & 7 & 1\\
        Probability & $r^2$ & $p^2 + 2pr$ & $q^2 + 2qr$ & $2pq$
    \end{tabular}
    \label{tab:blood_type}
\end{table}

Let $Y_1$, $Y_2$, $Y_3$ and $Y_4$ be the number of people with blood type O, A, B and AB respectively, and $Y = (Y_1, Y_2, Y_3, Y_4)$. Let $r$, $p$ and $q$ be the proportions of alleles O, A and B respectively within the population of interest. Since $r + p + q = 1$, we let $\theta = (p, q)$ be our target of inference. Pretending that the population size is fixed and that iid sampling was employed, $Y$ follows a multinomial distribution with $n =$ 4,464,349. Assuming homogeneous genetic mixing, the probability vector for $Y$ is $\pi = (r^2, p^2 + 2pr, q^2 + 2qr, 2pq)$.

Maximizing the likelihood in this model involves solving the score equation, a system of two 3rd-degree polynomials in $p$ and $q$. This can be done numerically, and gives estimates $p = 0.299$ and $q = 0.128$. These values match the ones given by \citet{Fuj78}. The information matrix and asymptotic covariance (inverse information matrix) are given by
%
\begin{align}
    I(\hat{\theta}) &= \begin{bmatrix}
        276 & 84.8\\
        84.8 & 584
    \end{bmatrix}\\
    \hat{\Sigma}_\mathrm{MLE} &= \begin{bmatrix}
        3.79 \cdot 10^{-3} & -5.49 \cdot 10^{-4}\\
        -5.49 \cdot 10^{-4} & 1.79 \cdot 10^{-3}
    \end{bmatrix}
\end{align}

\subsubsection{Complete Data}

The problem of gene frequency estimation would be much simpler if we could observe individuals' genotypes. We consider augmenting the observed data $Y$ by further classifying individuals by genotype. Let $X = (X_1, \ldots, X_6)$ be the genotypes of the individuals represented in Table \ref{tab:blood_type}. See Table \ref{tab:blood_type_complete}.

\begin{table}
    \centering
    \caption{Terminology and probabilities for our augmented version of the dataset in \citet{Fuj78}. We also give the blood type coded for be each genotype.}
    \begin{tabular}{c|cccccc}
        Genotype & OO & AO & AA & BO & BB & AB\\
        \hline
        Random Variable & $X_1$ & $X_2$ & $X_3$ & $X_4$ & $X_5$ & $X_6$\\
        Probability & $r^2$ & $2pr$ & $p^2$ & $2qr$ & $q^2$ & $2pq$\\
        Blood Type & O & A & A & B & B & AB
    \end{tabular}
    
    \label{tab:blood_type_complete}
\end{table}

Note that we can write $Y$ in terms of $X$. Specifically, $Y_1 = X_1$, $Y_2 = X_2 + X_3$, $Y_3 = X_4 + X_5$ and $Y_4 = X_6$. This corresponds to summing components of $X$ which correspond to the same blood type. The distribution of $X$ is multinomial, with the same sample size as $Y$, and probability vector given in Table \ref{tab:blood_type_complete}.

See Appendix \ref{app:blood_complete} for the complete data likelihood function and its derivatives.

\subsubsection{EM Algorithm}

The gene frequency estimation problem fits nicely into the EM algorithm framework. In this section, we present key quantities and results of our analysis. See Appendix \ref{app:blood}, especially part \ref{app:blood_miss}, for more details.

The EM objective function at iteration $k+1$ is
%
\begin{align}
    Q(\theta|\theta_k) &\equiv  \bE_{\theta_k}(n_O | y) \log r + \bE_{\theta_k}(n_A | y) \log p + \bE_{\theta_k}(n_B | y) \log q\\
    &=: \nu^{(k)}_O \log r + \nu^{(k)}_A \log p + \nu^{(k)}_B \log q
\end{align}
%
where $nu^{(k)}_O$, $\nu^{(k)}_A$ and $\nu^{(k)}_B$ are the expected number of O, A and B alleles respectively given $Y=y$ and $\theta = \theta_k$. Maximizing this objective function in $p$ and $q$ gives the following EM update function:
%
\begin{align}
    M(\theta_k) &= \begin{pmatrix}
        p_{k+1}\\
        q_{k+1}
    \end{pmatrix}\\    
    &= \begin{pmatrix}
        \nu^{(k)}_A / 2n\\
        \nu^{(k)}_B / 2n
    \end{pmatrix}
\end{align}
%
Starting with $\theta_0 = (1/3, 1/3)$ corresponding to equal proportions of the three alleles, Figure \ref{fig:blood_EM_traj} gives trajectories of the EM estimates for $p$ and $q$ using the data in Figure \ref{tab:blood_type}. These estimates converge quite quickly to the maximizer of the observed data likelihood.
%
\begin{figure}
    \centering
    \caption{Trajectory of EM estimates for $p$ and $q$ for the blood type example.}
    \label{fig:blood_EM_traj}
    \includegraphics[width=0.5\textwidth]{../plots/Blood_Type/EM_traj.pdf}
\end{figure}
%
Beyond computing the observed data MLE, we also need the standard error of this estimator. To this end, we compute the observed data information matrix using Louis' Method (see Proposition \ref{thm:info_decomp}). The asymptotic covariance matrix of our MLE is then the inverse of this information matrix. Omitting details (see Appendices \ref{app:blood_complete} and \ref{app:blood_miss}), both the observed data information matrix and asymptotic covariance match those obtained from the observed data likelihood. 
%


\section{The Monte Carlo EM Algorithm}

The Monte Carlo EM, or MCEM, algorithm was first proposed by \citet{Wei90}. This method proceeds by replacing the conditional expectation in the E-step of the EM algorithm with a Monte Carlo average. More precisely, at each iteration we generate observations from the conditional distribution of the missing data given the observed data, and average the complete data likelihood over this Monte Carlo sample. Formally, at a given iteration of the MCEM algorithm, let $X_1,\ldots, X_M$ be a Monte Carlo sample from the law of $X|Y=y$ with $\theta$ set to the value from the previous iteration, say $\theta_0$. Write
%
\begin{align}
    \hat{Q}(\theta|\theta_0) &= \sum_{i=1}^M w_i \ell_c(\theta; y, X_i)\\
    &:= \hat{\bE} \ell_c (\theta; y, X)
\end{align}
%
where the $w_i$ are sampling weights. \hl{Confirm that this operator notation isn't contradicted elsewhere.} Under iid sampling we simply get $w_i = M^{-1}$ for every $i$, but more intricate sampling schemes may have more complicated weights. The estimate of $\theta$ is then the maximizer of the MCEM objective function: $\hat{\theta} = \argmax_\theta \hat{Q}(\theta|\theta_0)$. Write $\hat{\theta}_{k-1}$ for the $k$th MCEM estimate. 

Provided that a valid sampling scheme is available for the missing data distribution, we can use Proposition \ref{thm:info_decomp} to estimate the observed data information matrix. 

\begin{proposition}
    Under the conditions of Proposition \ref{thm:info_decomp}, \hl{as well as any required for the sampler,} we get
    %
    \begin{align}
        - \hat{\bE}_{\hat{\theta}} \nabla^2 \ell_c(\hat{\theta}) - \hat{\bE}_{\hat{\theta}} S_c(\hat{\theta}) S_c(\hat{\theta})^T \rightarrow I(\theta)
    \end{align}
    %
    \hl{Under stronger conditions,} we also get asymptotic normality with variance obtained from importance sampling analysis.
\end{proposition}

The MCEM algorithm has the advantage of circumventing the challenge of computing potentially intractable conditional expectations for the EM algorithm. However, this analytical simplification does come at the cost of introducing some new computational problems. In this section, we outline the main problems faced by the MCEM algorithm and present various solutions which have been proposed in the literature. We focus primarily on practical aspects of the MCEM algorithm; see \citet{Nea13} for a survey of theoretical considerations.

Two problems which have received considerable attention in the literature are how to choose the Monte Carlo sample size at each iteration, and how to decide when to terminate the MCEM algorithm. These were identified as early as \citet{Wei90}, but did not receive systematic treatment until later. We here give a brief overview of different authors' approaches to solving these two problems, and spend the rest of this section going into more detail on each method individually. \citet{Wei90} suggest examining a plot of the parameter estimates across iterations, and either terminating or increasing the Monte Carlo size when the plot appears to stabilize. \citet{Cha95} use a pilot study to choose the Monte Carlo sample size, and terminate when a confidence interval for the improvement of the observed data log-likelihood between successive iterations contains zero. \citet{Boo99} frame each MCEM iteration as an M-estimation problem targeting the deterministic EM update. They increase the Monte Carlo size if an asymptotic confidence interval for the EM update contains the previous iteration's parameter estimate, and terminate when multiple successive iterations' estimates have sufficiently small relative error. \citet{Caf05} build confidence bounds for the increment in the EM objective function at each iteration of the MCEM algorithm. They increase the Monte Carlo size until the lower bound is positive and terminate when the upper bound is sufficiently small.

In the rest of this section, we give more detail on each of the implementations introduced above. In each subsection, we also describe how the method being discussed can be used to analyze the blood type dataset described in Section \ref{sec:eg-genes}. The relevant conditional distribution and likelihood calculations are described in Appendix \ref{app:blood_miss}.

\subsection{Early Work (\citealp{Wei90})}

In their seminal work, \citet{Wei90} propose the MCEM algorithm and present a simple implementation. They illustrate that the complete data gradient and Hessian are easily obtained at each iteration from the Monte Carlo sample and, following \citet{Lou82}, give an estimator for the observed data information matrix. Regarding convergence, \citeauthor{Wei90} recommend plotting the parameter estimates across iterations and stopping when the estimates appear to stabilize around some constant. When this stabilization is detected, one can either declare convergence and stop, or increase the Monte Carlo size and continue iterating until the estimate trajectory again stabilizes.

In order to apply the MCEM algorithm to estimate allele frequencies in the blood type problem, we must specify the number of iterations, $K$, and the Monte Carlo size for each iteration, $M$. Starting conservatively, we use $K=50$ and $M=100$. Figure \ref{fig:blood_naive_MCEM_traj1} gives trajectories of the MCEM estimates for $p$ and $q$. These estimates appear to converge quickly to a stationary mean, but there is still some uncertainty around this mean. As such, we run MCEM for another 20 iterations with $M=1000$, staring with the final value from our first run. See Figure \ref{fig:blood_naive_MCEM_traj2}. The trajectories from our second run are much more stable around their means. We use the final values from these trajectories as our estimates: $\hat{p} = 0.298$ and $\hat{q} = 0.128$. These values closely match the maximizer of the observed data likelihood.

\begin{figure}
    \centering
    \caption{Trajectory of MCEM estimates for $p$ and $q$ for the blood type example.}
    \label{fig:blood_naive_MCEM_traj}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width = 1\textwidth]{../plots/Blood_Type/naive_MCEM_traj1.pdf}
        \caption{$M=100$}
        \label{fig:blood_naive_MCEM_traj1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width = 1\textwidth]{../plots/Blood_Type/naive_MCEM_traj2.pdf}
        \caption{$M=1000$}
        \label{fig:blood_naive_MCEM_traj2}
    \end{subfigure}
   
\end{figure}

\subsection{Running a Pilot Study}



\subsection{Uncertainty Quantification for the Parameter Estimate (\citealp{Boo99})}

Building on the ideas of \citeauthor{Wei90}, \citet{Boo99} seek to start the MCEM algorithm with a small Monte Carlo size, and add more observations only when the parameter estimates are no longer changing discernibly across iterations. To this end, they recommend building a confidence interval for the EM update based on the Monte Carlo variability of the MCEM update at each iteration. If this interval contains the previous iteration's parameter estimate, then the parameter updates are too small relative to the amount of Monte Carlo variability and more samples are required. Similarly, \citeauthor{Boo99} recommend assessing convergence by checking for small relative error in the parameter updates. To account for the possibility of Monte Carlo variability leading to two consecutive estimates being similar before the algorithm has `converged', the authors suggest waiting until the relative error is small for three consecutive iterations.

The confidence interval used to quantify Monte Carlo uncertainty within an iteration is obtained by framing the parameter update as the solution of an M-estimation problem. This allows us to inherit the desirable properties of M-estimators; specifically, asymptotic normality. See, e.g. \citet{van98}. Following the usual M-estimator construction and assuming that the relevant regularity conditions hold, we are able to estimate the asymptotic variance of the MCEM parameter estimator at each iteration. Note that this standard error is based on the Monte Carlo variability within an iteration; it does not measure sampling variability due to the observed data. 

More formally, write $\tilde{\theta}_k$ for the EM update based on $\hat{\theta}_{k-1}$. Note that $\hat{\theta}_{k-1}$ is held fixed here and in the next subsection. \hl{Analysis of a single MCEM iteration is done conditional on the previous iteration (awk?).} Unless stated otherwise, all expectations are taken with $\theta = \hat{\theta}_{k-1}$. Assuming sufficient smoothness and moment conditions, we get the following expression for the MCEM update:
%
\begin{align}
    \sqrt{M}(\hat{\theta}_k - \tilde{\theta}_k) &= - \sqrt{M} \left[ \nabla^2 Q(\tilde{\theta}_k|\hat{\theta}_{k-1})\right]^{-1} \left[\nabla \hq(\tilde{\theta}_k|\hat{\theta}_{k-1}) \right] + o_p(1) \label{eq:th_as_dist}
\end{align}
%
where $M$ is the Monte Carlo size and $\nabla$ denotes differentiation with respect to the left argument of $Q$ or $\hat{Q}$. Note that the first expression on the right-hand side is the inverse Hessian of the EM objective function (fixed) while the second is the gradient of the MCEM objective function (an average). Thus, $\hat{\theta}_k$ is asymptotically normal with asymptotic variance
%
\begin{align}
    &\left[ \nabla^2 Q(\tilde{\theta}_k|\hat{\theta}_{k-1})\right]^{-1} \bV \left[ S_c (\tilde{\theta}_k) | Y=y \right] \left[ \nabla^2 Q(\tilde{\theta}_k|\hat{\theta}_{k-1})\right]^{-1}\\
    &\approx \left[ \nabla^2 \hat{Q}(\hat{\theta}_k|\hat{\theta}_{k-1})\right]^{-1} \hat{\bE} \left[ S_c(\hat{\theta}_k) S_c(\hat{\theta}_k)^T | Y=y \right] \left[ \nabla^2 \hat{Q}(\hat{\theta}_k|\hat{\theta}_{k-1})\right]^{-1} 
\end{align}
%
where $S_c$ is the complete data score vector, and $\hat{\bE}$ is the Monte Carlo average over the missing data, with $\hat{\theta}_k$ held fixed \hl{(remove comma?)}. Note that there is no first moment term in the conditional variance of $S_c$ because $\hat{\theta}_k$ is a maximizer of $\hat{\bE} [\ell_c |Y=y]$.

Based on the above discussion, we can build an asymptotic confidence interval for $\tilde{\theta}_k$, the EM update based on the MCEM estimate from iteration $k$. \citeauthor{Boo99} recommend checking whether this interval contains $\hat{\theta}_{k-1}$ and, if so, increasing $M$ for the next iteration. Specifically, they suggest starting the next iteration with $M/r$ more points, with $r = 3,4$ or $5$ working well in their examples \hl{(this whole discussion is pretty awkward)}.

To assess convergence of the MCEM algorithm, \citeauthor{Boo99} present two criteria. The first is a familiar measure of relative error in parameter estimates between consecutive iterations:
%
\begin{align}
    \max_j \left( \frac{\left| \hat{\theta}_{k, j} - \hat{\theta}_{k-1,j} \right|}{\left| \hat{\theta}_{k-1,j} \right| + \delta_1} \right) < \delta_2 \label{eq:Boo99_tol}
\end{align}
%
where $\delta_1$ and $\delta_2$ are small positive constants, and the subscript $j$ ranges over components of $\theta$. \citeauthor{Boo99} suggest using $\delta_1 = 10^{-3}$ and $\delta_2$ between $2 \cdot 10^{-3}$ and $5 \cdot 10^{-3}$. See \citep{need} (probably \citealp{Sea06}, p.\ 436, or \citealp{Mar63}) for why condition (\ref{eq:Boo99_tol}) has this particular form.

Alternatively, since \citeauthor{Boo99} apply their method to the analysis of generalized linear mixed models, where pathologies may arise due to parameter estimates being too close to a boundary, they propose a second stopping rule:
%
\begin{align}
    \max_j \left( \frac{\left| \hat{\theta}_{k, j} - \hat{\theta}_{k-1,j} \right|}{\sqrt{\bV \hat{\theta}_{k-1,j}} + \delta'_1} \right) < \delta'_2 \label{eq:Boo99_tol2}
\end{align}

\hl{How do we estimate the variance here?} The purpose of condition (\ref{eq:Boo99_tol2}) is to detect when estimated variance components are very close to zero and the numerical precision needed to satisfy condition (\ref{eq:Boo99_tol}) requires a prohibitive amount of computation.

\subsection{Uncertainty Quantification for the Objective Function \citep{Caf05}}

The approach of \citet{Caf05} is similar in spirit to that of \citet{Boo99}. Both sets of authors seek to quantify Monte Carlo uncertainty in the MCEM algorithm as an approximation to the EM algorithm. The difference is that where \citeauthor{Boo99} measure uncertainty in the parameter estimate, \citeauthor{Caf05} focus on uncertainty in the objective function. Specifically, \citeauthor{Caf05} base their analysis on asymptotic normality of the MCEM increment:
%
\begin{proposition}
    \label{thm:Caf_normality}
    Let $\Delta \hat{Q}(\hat{\theta}_k|\hat{\theta}_{k-1}) = \hat{Q}(\hat{\theta}_{k-1}|\hat{\theta}_{k-1}) - \hat{Q}(\hat{\theta}_k|\hat{\theta}_{k-1})$. Define $\Delta Q(\hat{\theta}_k|\hat{\theta}_{k-1})$ similarly. Let $M_k$ be the Monte Carlo size at iteration $k$. Then, \hl{assuming some regularity conditions},
    %
    \begin{align}
        \sqrt{M_k} \left[ \Delta \hat{Q}(\hat{\theta}_k|\hat{\theta}_{k-1}) - \Delta Q(\hat{\theta}_k|\hat{\theta}_{k-1}) \right] \rightsquigarrow N(0, \Sigma_k)
    \end{align}
    %
    As $M_k \rightarrow \infty$, where $\Sigma_k$ is an asymptotic covariance matrix.
\end{proposition}

\begin{proof}
    Following \citet{Caf05}, we write
    %
    \begin{align}
        \sqrt{M_k} \left[ \Delta \hat{Q}(\hat{\theta}_k|\hat{\theta}_{k-1}) - \Delta Q(\hat{\theta}_k|\hat{\theta}_{k-1}) \right] &= \sqrt{M_k} \left[ \Delta \hat{Q}(\theta_k|\hat{\theta}_{k-1}) - \Delta Q(\theta_k|\hat{\theta}_{k-1}) \right] \nonumber \\
        & + \sqrt{M_k} \left[ \Delta \hat{Q}(\hat{\theta}_k|\hat{\theta}_{k-1}) - \Delta Q(\theta_k|\hat{\theta}_{k-1}) \right]\\
        & + \sqrt{M_k} \left[ \Delta \hat{Q}(\theta_k|\hat{\theta}_{k-1}) - \Delta Q(\hat{\theta}_k|\hat{\theta}_{k-1}) \right]\\
        &=: A_k + B_k + C_k
    \end{align}
    %
    First, note that $A_k$ depends on the current Monte Carlo sample only through $\hat{Q}$, and is thus asymptotically normal by the ordinary Central Limit Theorem. However, $B_k$ and $C_k$ require a more careful analysis.

    \textbf{Use Taylor's Theorem when you're more awake.}
\end{proof}

Provided that we are able to estimate $\Sigma_k$, Proposition \ref{thm:Caf_normality} allows us to build asymptotic confidence intervals for the EM increment, $\Delta Q$. Recall that in Section \ref{sec:GEM}, we defined the Generalized EM algorithm by requiring that $\Delta Q \geq 0$, and showed that this requirement guarantees the ascent property. While the stochastic nature of the MCEM algorithm makes it impossible to guarantee that the EM increment is positive, we are able to use Proposition \ref{thm:Caf_normality} to construct asymptotic confidence bounds for $\Delta Q$. Provided that we can estimate $\Sigma_k$, we can then \hl{be reasonably confident that $\Delta Q > 0$ (awk)}.

Estimating the asymptotic variance under iid or importance sampling is fairly straightforward. Importance sampling however, is somewhat more complicated; particularly when a normalizing constant must be estimated. \citeauthor{Caf05} give a formula for importance sampling based on the Delta Method. They also give some guidance for calculating standard errors based on MCMC sampling, \hl{which we do not go into here. See Section XX for some details}. 

We now return to the key MCEM problems of choosing the Monte Carlo size and when to terminate. For the former, \citeauthor{Caf05} advise constructing a lower confidence limit for the EM increment, $\Delta Q$. If this limit is positive, then we proceed to the next iteration. If not, then we augment the Monte Carlo sample at the current iteration (with, say, $M_k/r$, with $r$ some small positive integer as in \citealp{Boo99}), and compute a new confidence bound. At the next iteration, \citeauthor{Caf05} advise using a starting Monte Carlo sample which is at least as large as the final sample from the previous iteration. \hl{In fact, a larger sample may be required based on extrapolating the MC variability from the previous iteration (clarify; I don't fully understand what they're doing here).}

\citeauthor{Caf05} base their termination criterion on stopping when there is evidence that the algorithm is no longer yielding sufficient improvement in the EM objective function. Specifically, they start by choosing a tolerance, $\tau>0$, then calculate an upper confidence limit for the EM increment at each iteration. If this upper confidence limit is below $\tau$, then we declare that there is little room for improvement left in the EM objective, and terminate our algorithm.

\begin{comment}
\subsection{Quantifying Monte Carlo Uncertainty}

In their seminal work, \citet{Wei90} highlight two important challenges with implementing this method: choosing the Monte Carlo sample size at each iteration, and deciding when to terminate the algorithm. It turns out that solutions to these two problems are often connected via their link to the uncertainty in our Monte Carlo conditional expectations. The recommendations given by \citeauthor{Wei90} on how to solve these problems are mostly informal, but much of the later work on the MCEM algorithm centers around developing more precise solutions \hl{(too strong of a statement?)}.

\citet{Cha95} present an alternative method for choosing the Monte Carlo sample size based on starting with a pilot study and using information near the optimal parameter estimate\footnotemark to choose a Monte Carlo size for the rest of the analysis. In contrast to \citet{Wei90}, \citeauthor{Cha95} use a fixed Monte Carlo size for their analysis.

\footnotetext{\citet{Cha95} present an identity which expresses the observed data log-likelihood ratio as the conditional expectation of the corresponding complete data log-likelihood ratio. Replacing the conditional expectation with a Monte Carlo average gives a natural estimate of the observed data log-likelihood ratio. This approach closely resembles the Monte Carlo Maximum Likelihood method of \citet{Gey94}. See \hl{Section ???}.}

The method of \citet{Boo99} centers on treating each iteration of the MCEM algorithm as an M-estimation problem targeting the deterministic EM update. This framework is quite natural, as an iteration of the MCEM algorithm consists of maximizing a Monte Carlo approximation to the EM objective function. Provided that certain regularity conditions are satisfied (see, e.g., \citealp{van98}), we can estimate the asymptotic standard error of the MCEM update as an estimate of the EM update with the same starting value, with the sampling variability induced by Monte Carlo simulation. \citeauthor{Boo99} then recommend constructing an asymptotic confidence set for the EM update, and increasing the Monte Carlo size if this confidence set contains the previous iteration's parameter estimate\footnotemark.

\footnotetext{For myself: As far as I can tell, in their paper, \citeauthor{Boo99} don't suggest that you check whether augmenting the Monte Carlo size produces a confidence set which does not contain the previous iteration's estimate. I think they're just suggesting you increase M and proceed with the next iteration. This differs from \citet{Caf05} who require that we keep checking and augmenting M until the condition is satisfied. \hl{I don't think this needs to go in the paper, but it's of some interest if you want to implement the method of} \citeauthor{Boo99}}
\end{comment}

\section{Simulation}

A further obstacle to implementing the MCEM algorithm which was not addressed in the previous section, is how to generate the Monte Carlo sample. It is in general a hard problem to simulate from arbitrary conditional distributions. In fact, much of the Bayesian Computation literature centers around this problem (see, e.g., \citealp{Gel13}). In this section, we discuss several methods for simulating the necessary observations at each step of the MCEM algorithm.

\subsection{Importance Sampling}

\hl{The beginning of this section is pretty rambly. It will need to be tightened-up.}

Importance sampling is a general framework for approximating expectations under an intractable distribution. The key idea is to replace the $d \bF$ integral of a function, $\phi$, with the $d \bG$ integral of $\phi \cdot (d \bF / d\bG)$. We call $\bF$ the target distribution and $\bG$ the proposal distribution. For ease of exposition, we assume that both $\bF$ and $\bG$ have densities with respect to a common base measure; call these densities $f$ and $g$ respectively.

The importance sampling literature is vast, and we cannot hope to summarize it here in its entirety. A classic reference on importance sampling and other Monte Carlo methods is the book by \citet{Rob04}; particularly Chapters 3 and 4. Chapter 8 of the book by \citet{Cho20} gives a more current overview of importance sampling, with a focus on its application to Sequential Monte Carlo. \citet{Aga17} give a survey paper level treatment of some more theoretical considerations of importance sampling.

\hl{Re-write intro after writing body}. In this section, we focus on the problem of choosing a proposal distribution, $\bG$. We begin with a more formal description of the importance sampling estimator some theory which helps inform the choice of $\bG$, then give some practical guidance. 

When doing importance sampling, we seek to estimate $\phi = \int h f $, for some test function, $h$. To do so, observe that we can re-write $\phi = \int h (f/g) g$, for any proposal density $g$, such that the integral is finite. Typically, we choose $g$ so that it is easy to sample from; let $X_1,\ldots,X_M$ be such a sample. We estimate $\phi$ by $\hat{\phi} = \hat{\bG} h := (1/M) \sum_{i=1}^M w_i h(X_i)$, where $w_i = f(X_i)/g(X_i)$ is referred to as the importance weight for observation $i$. Note that $\bG \hat{\phi} = \int (f/g) h g = \phi$, so  $\hat{\phi}$ is unbiased. It is similarly straightforward to show that the variance of $\hat{\phi}$ is $M^{-1} \bG w^2 h^2$. 

It is often the case that we only know the target density, $f$, up to a proportionality constant, $\alpha_f$. The same may also be true of the proposal density, $g$, where we may have a desirable proposal distribution in mind but may only be able to compute its density up to a proportionality constant, $\alpha_g$. Write $\tilde{f}$ and $\tilde{g}$ for the un-normalized densities. Paralleling our previous development, we write $\tilde{w} = \tilde{f} / \tilde{g}$ for the ratio of the un-normalized densities. Note that $\tilde{w}$ differs from the true likelihood ratio by a third proportionality constant, $\alpha = \alpha_f / \alpha_g$. In order to estimate $\phi$, we must also estimate the constant $\alpha$. To this end, note the following two identities:
%
\begin{align}
    \bG h \tilde{w} &= \frac{\phi}{\alpha}\\
    \bG \tilde{w} &= \frac{1}{\alpha}
\end{align}
%
This suggests using a ratio estimator for $\phi$:
%
\begin{align}
    \tilde{\phi} &:= \frac{\hat{\bG} h \tilde{w}}{\hat{\bG} \tilde{w}} \label{eq:SNIS}
\end{align}
%
The estimator in (\ref{eq:SNIS}) is referred to as the self-normalized importance sampling (SNIS)\index{Self-Normalized Importance Sampling} estimator of $\tilde{\phi}$. Although the numerator and denominator of (\ref{eq:SNIS}) are perfectly well-behaved objects, the full SNIS estimator is best studied in general terms using asymptotic methods. Specifically, noting that $\tilde{\phi}$ is a ratio of averages, a routine analysis using the CLT, LLN and Slutsky's Theorem \citep[p.91]{Cho20} gives (under the usual regularity conditions\footnotemark):
%
\begin{align}
    \sqrt{M} (\tilde{\phi} - \phi) \rightsquigarrow N(0, \sigma^2_{SNIS})
\end{align}
%
where $\sigma^2_{SNIS} = \bG ((h - \phi)^2 \tilde{w}^2) / (\bG \tilde{w})^2 =\alpha^2 \bG ((h - \phi)^2 \tilde{w}^2)$. It is common to define the normalized weights as $\tilde{w}_i / \hat{\bG} \tilde{w}$\index{Normalized Importance Weights}, which I denote (non-standardly) by $\mathring{w}_i$. Thus, we can define $\tilde{\bG} h := \tilde{\phi} = \sum \mathring{w}_i h(X_i)$.

\footnotetext{\textbf{Finite second moment assumptions à la CLT.}}

One way to assess the performance of an importance sample is by computing the so-called ``effective sample size''\index{Effective Sample Size}. This quantity is defined as $\mathrm{ESS} := (\sum \wcirc_i^{2})^{-1}$.

\subsection{Choosing a Proposal Distribution}

When designing an importance sampling scheme to solve a particular problem, it is important to choose an appropriate proposal distribution. The sense in which a proposal distribution should be appropriate can be understood in a few different ways. If multiple test functions must be integrated using the same proposal distribution, or even the same sample, it makes sense to consider a worst-case error analysis over some class of test functions and to choose a proposal for which the bound is not too large. In contrast, if the goal is to estimate the expectation of a single test function, the proposal can be chosen to better facilitate estimation of the particular integrand. In the former case, we choose our proposal distribution based on its relationship with the target distribution alone, while in the latter case we make our choice based on the target distribution and the test function.

\citet{Aga17} give a very readable overview of some bounds for the worst-case error of an importance sampling scheme. These error bounds are simpler if we restrict attention only to bounded test functions, although similar error bounds also exist for unbounded test functions, provided that certain integrability criteria are met. Focusing on bounded test functions for simplicity, the fundamental quantity for the error bounds in \citeauthor{Aga17} is $\rho := \bG \tilde{w}^2 / (\bG \tilde{w})^2$. Specifically, for test functions bounded in absolute value by 1, the worst-case $\mathcal{L}^2$ error is less than $4 \rho / M$, where $M$ is the size of the importance sample. 

When approximating the $\bF$-expectation of a single test function, it is possible to choose the proposal distribution based on this test function for increased performance. In particular, given a target density, $f$, and a test function, $\phi$, there is an optimal proposal density given by the following proposition.
%
\begin{proposition}
    \label{thm:imp_samp}
    Let $f$ be a density and $\phi$ be a test function. Let $g_* = f |\phi| / \bF |\phi|$, and $g$ be any other proposal density. Then $\bV_{g_*} w_* \phi \leq \bV_g w \phi$, where $w_* = f/g_*$ and $w = f/g$.
\end{proposition}

\begin{proof}
    See the proof of Proposition 8.2 in \citet{Cho20}.
\end{proof}
%
Unfortunately, this optimal proposal density is rarely useful in practice, since evaluating $g_*$ requires computing $\bF |\phi|$. While it's not exactly equal to the quantity we are trying to approximate, $\bF |\phi|$ is unlikely to be much easier to compute.

Although Proposition \ref{thm:imp_samp} does not tell us exactly what to do in practice, this result does offer some guidance on how to choose proposal distributions. Specifically, we should choose $g$ to be positively correlated with $f |\phi|$. That is, $g$ should place most of its mass in regions where $f \phi$ is large in magnitude.

\hl{Maybe write something about exponential tilting, although that might be mostly for rare event simulation.}

\hl{Add some discussion around what importance sampling looks like in MCEM.}

\subsection{Particle-Based Methods}

\subsection{Markov Chain Monte Carlo}

The literature on Markov Chain Monte Carlo, or MCMC, is vast. We cover only the basics here and refer the reader to any number of excellent introductions, such as \citet{Gel13} or \citet{Rob04}. The core idea is to construct a Markov Chain from which we can simulate, and which has stationary distribution equal to the target distribution. Popular methods to construct such a Markov Chain are Gibbs sampling and the Metropolis-Hastings algorithm. 

Both Gibbs sampling and \mh start with a random variable, $X = (X_1, \ldots, X_d)$, which we wish to simulate. Let $f$ be the density of $X$. These methods proceed by iteratively simulating draws of the vector $X$ from some distribution based on the draw from the previous iteration. 

Gibbs sampling\index{Gibbs Sampling} is based on successively sampling each component of $X$ conditional on all the other components. The order in which this conditioning is performed is a bit subtle, however. When generating $X_i$, we condition on the values of $X_1,\ldots, X_{i-1}$ from the current iteration and the values of $X_{i+1}, \ldots, X_d$ from the previous iteration. Once we reach the end of $X$, we start a new iteration.  More generally, we can group the components of $X$, and simulate an entire group conditional on the others, provided that we follow the structure outlined above \hl{(awk)}.

The Metropolis-Hastings algorithm\index{Metropolis-Hastings Algorithm} closely resembles rejection sampling. We start each iteration by generating a candidate value of $X$ from some proposal distribution (this distribution may depend on the previous iteration's value of $X$). Write $J(x | x_0)$ for the proposal density, where $x_0$ is the value of $X$ from the previous iteration. Next, we define an acceptance probability, $r := f(x) J(x_0 | x) / f(x_0) J(x | x_0)$, and accept the proposed value of $X$ with probability $r \wedge 1$. With probability $(1-r) \vee 0$, we reject the proposed $x$ and instead set the current iteration's value to $x_0$. We then proceed to the next iteration. Note that rejecting still adds an observation to our Monte Carlo sample, this value just happens to be identical to the one proceeding it.



\section{Alternatives to the MCEM Algorithm}

In this section, we outline a few alternatives to the MCEM algorithm for maximizing the likelihood of an incomplete dataset. Examples include the Monte Carlo Maximum Likelihood method of \citet{Gey94}, and Variational Inference \citep{Ble17, Tsi08}.

\subsection{Monte Carlo Maximum Likelihood}



\subsection{Stochastic Approximation}

\subsection{Variational Methods}

\textbf{This section was written by memory (i.e. without looking anything up). The idea is to more efficiently get words on the page, then I can fix things later.}

Variational inference is a set of methods for approximating intractable densities \citep{need}. The literature on variational inference is vast. See \citet{Ble17} for a recent survey paper. \citet[Section 13.7]{Gel13} give a textbook-level overview focusing on applications to Bayesian inference. Numerous authors have discussed the connection between variational inference and the EM algorithm; see, e.g., \citet{Nea98,Tsi08}.

The central idea of variational inference is to frame the target density as the exact solution of a functional optimization problem, where the decision variable typically ranges over densities. The domain of functions is then restricted to some set which is easier to work with. Finally, optimization is performed over this restricted class of functions. The result of this optimization is then our approximation to the target density. \hl{This discussion doesn't make it clear whether our estimator is the optimizer or the optimal objective function.} 

More generally, the optimization problem described above may be just one in a sequence of problems which may form part of an iterative algorithm. Herein lies the connection to the EM algorithm. In the M-step of EM, we compute expectations with respect to a particular conditional distribution. To embed this procedure in the variational inference framework, we define an optimization problem whose solution is the same conditional distribution.

We now give some details. First, let $q$ be some probability density for the missing data, $X$. We can write the observed data log-likelihood as:
%
\begin{align}
    \ell(\theta; y) &= \log f_c(y, x; \theta) - \log f_m(y, x; \theta)\\
    &= \log \left[ \frac{f_c(y, x; \theta)}{q(x)} \right] - \log \left[ \frac{f_m(y, x; \theta)}{q(x)} \right]\\
    &= \bQ \log \left[ \frac{f_c(y, X; \theta)}{q(X)} \right] - \bQ \log \left[ \frac{f_m(y, X; \theta)}{q(X)} \right] \label{eq:variational_expectation}\\
    &=: F(q, \theta) + \mathrm{KL}(q \rightarrow f_m(\theta)) \label{eq:variational_decomp}\\
    & \geq F(q, \theta)
\end{align}
%
where line (\ref{eq:variational_expectation}) holds because the left-hand side does not depend on $x$, and the last line follows from non-negativity of the KL divergence. The first term in line (\ref{eq:variational_decomp}) is called the ``evidence lower-bound'' (ELBO), as well as the ``variational free energy'', depending on the field of application \citep{need}. The latter name comes from an analogue in physics \citep{need}. The former name comes from applications to Bayesian inference, where the observed data log-likelihood can be seen as an evidence if we take $Y$ to be the data and $X$ to be the parameters. 

The EM algorithm can be re-framed as alternately maximizing $F$ with respect to $q$ and $\theta$. To see why, first recall that the KL divergence between two distributions is non-negative, and is zero if and only if the two distributions are equal. Starting at a fixed parameter value, $\theta_0$, maximizing $F$ with respect to $q$ is accomplished by setting $q(x) = f_m(y, x; \theta_0)$, as this minimizes the KL divergence in (\ref{eq:variational_decomp}) and the left-hand side is constant in $q$. The resulting ELBO is equal to $Q(\theta|\theta_0) + \xi$, where $Q$ is the EM objective function and $\xi$ is constant in $\theta$. Maximizing the ELBO in $\theta$ is therefore equivalent to maximizing the EM objective function. This maximization gives a new parameter value, which is used as input to the next iteration. See Theorem 1 of \citet{Nea98} for a more formal proof.

While the EM algorithm is obtained by maximizing $q$ over an unrestricted class of densities, other procedures can be formulated by restricting this class. One popular example is the ``mean-field'' approximation, which involves optimizing over the class of densities which factor over their arguments. That is, the class of functions $\mathcal{Q}_{MF}:=\{ \mathrm{Densities\ } q: q(X_1, \ldots, X_p) = \prod_{j=1}^p q_j(X_j) \}$. 

A major advantage of the mean-field approximation is that an iterative algorithm exists for finding the density, $q$, which maximizes the ELBO. This algorithm performs coordinate ascent, and the coordinate updates are closely related to computation of the full conditional distributions in Gibbs sampling \citep{need}. Write $q^{(k)} = \prod q_j^{(k)}$ for the current value of $q$, and $\bQ^{(k)}_{-j}$ for expectation with respect to all the missing variables except $j$, with distributions from $q^{(k)}$ (\hl{awk?}). The update formula is
%
\begin{align}
    q_j^{(k+1)} &\propto \exp \left[ \bQ^{(k)}_{-j} \ell_c(y, x_j, X_{-j}) \right] \label{eq:var_inf_update}
\end{align}
%
where $X_{-j}$ is all the missing variables other than $X_j$. See Section 2.4 of \citet{Ble17} for a derivation of (\ref{eq:var_inf_update}). The overall algorithm consists of repeatedly cycling through updating each coordinate's distribution until some convergence criterion is met (\hl{how is convergence assessed?}).

Note that, so far, our discussion of how to compute the mean-field approximate density for $X$ has not addressed $\theta$. To apply mean-field variational inference to EM-type problems, we substitute the mean-field density into the ELBO and maximize over $\theta$. This new value of $\theta$ is fed back into ($\ref{eq:var_inf_update}$), giving us a different complete data likelihood function and, hence, a new optimal density.


\newpage

\begin{appendices}
    \section{Likelihood for Gene Frequency Estimation}
    \label{app:blood}

    In this appendix, we present details for the analysis of our example of estimating gene frequency. See Section \ref{sec:eg-genes} for formulation of the model and definition of notation.

    \subsection{Observed Data Likelihood, Score and Information}
    \label{app:blood_obs}

    Let $\pi_i$ be the probability of blood type $i$. The observed data likelihood for our model can be written as follows:
    %
    \begin{align}
        \ell(\theta; y) &= \log \begin{pmatrix} n \\ y \end{pmatrix} + \sum y_i \log \pi_i(\theta)\\
        & \equiv \sum y_i \log \pi_i\\
        &\equiv 2 y_1 \log r + y_2 \log(p^2 + 2pr) + y_3 \log(q^2 + 2qr) + y_4 \log pq
    \end{align}
    where we use $\equiv$ to denote equality up to additive constants which do not depend on $\theta$.

    Differentiating $\ell$ wrt $\theta$ gives the observed data score.
    %
    \begin{align}
        S(\theta; y) &= \begin{pmatrix}
            \partial_p \ell(\theta; y)\\
            \partial_q \ell(\theta; y) 
        \end{pmatrix} \mathrm{, where}\\
        \partial_p \ell(\theta; y) &= - \frac{2 y_1}{r}  + \frac{2r y_2}{p^2 + 2pr}  - \frac{2q y_3}{q^2 + 2qr}  + \frac{y_4}{p} \label{eq:gene_obs_score1}\\
        \partial_q \ell(\theta; Y) &= - \frac{2 y_1}{r}  - \frac{2p y_2}{p^2 + 2pr}  + \frac{2r y_3}{q^2 + 2qr}  + \frac{y_4}{q} \label{eq:gene_obs_score2}
    \end{align}
    %
    Solving the score equation, $S(\theta) = 0$, thus reduces to solving a system of two polynomials in $p$ and $q$. Since $p$ and $q$ are proportions, we reject any roots outside the unit simplex.

    Differentiating $\ell$ again and multiplying by $-1$ gives the observed data information matrix. To simplify notation, let $p_y = p^2 + 2pr$ and $q_y = q^2 + 2qr$.
    %
    \begin{align}
        I(\theta;y) &= - \begin{bmatrix}
            \partial^2_p \ell(\theta; y) & \partial_{p,q} \ell(\theta; y)\\
            \partial_{p,q} \ell(\theta; y) & \partial^2_q \ell(\theta; y)
        \end{bmatrix} \mathrm{, where}\\
        \partial^2_p \ell(\theta; y) &=  \frac{2y_1}{r^2} + \frac{2 y_2 (p_y + 2r^2)}{p_y^2} + \frac{4 y_3 q^2}{q_y^2} + \frac{y_4}{p^2}\\
        \partial_{p,q} \ell(\theta; y) &=  \frac{2y_1}{r^2} + \frac{2 y_2 p^2}{p_y^2} + \frac{2 y_3 q^2}{q_y^2}\\
        \partial^2_q \ell(\theta; y) &=  \frac{y_1}{r^2} + \frac{4 y_2 p^2}{p_y^2} + \frac{2 y_3 (q_y + 2r)}{q_y^2} + \frac{y_4}{q^2}
    \end{align}
    %
    The asymptotic standard error of our MLE is $I^{-1}$, evaluated at the estimate.

    \subsection{Complete Data Likelihood, Score and Information}
    \label{app:blood_complete}

    The complete data distribution for our model can be written as follows. Write $\rho_i$ for the probability of genotype $i$. See Table \ref{tab:blood_type_complete} for the values of these probabilities.
    %
    \begin{align}
        \ell_c(\theta; y,x) &= \log \begin{pmatrix} n \\ x \end{pmatrix} + \sum x_i \log \rho_i(\theta)\\
        & \equiv \sum y_i \log \rho_i\\
        &\equiv 2 x_1 \log r + x_2 \log pr + 2 x_3 \log p + x_4 \log qr + 2 x_5 \log q + x_6 \log pq\\
        &= (2 x_1 + x_2 + x_4) \log r + (x_2 + 2 x_3 + x_6) \log p + (x_4 + 2 x_5 + x_6) \log q\\
        &= n_O \log r + n_A \log p + n_B \log q
    \end{align}
    %
    where $n_O$, $n_A$ and $n_B$ are the number of times allele O, A and B arise respectively in the sampled genotypes. Note that $\ell_c$ depends on $y$ only through $x$, so we suppress $y$ from our notation for complete data quantities. The complete data score function is
    %
    \begin{align}
        S_c(\theta; x) &= \begin{pmatrix}
            \partial_p \ell_c(\theta; x)\\
            \partial_q \ell_c(\theta; x) 
        \end{pmatrix} \mathrm{, where}\\
        \partial_p \ell_c(\theta; x) &= \frac{x_2 + 2 x_3 + x_6}{p} - \frac{2x_1 + x_2 + x_4}{r} = \frac{n_A}{p} - \frac{n_O}{r} \label{eq:comp_score1}\\
        \partial_p \ell_c(\theta; x) &= \frac{x_4 + 2 x_5 + x_6}{q} - \frac{2x_1 + x_2 + x_4}{r} = \frac{n_B}{q} - \frac{n_O}{r} \label{eq:comp_score2}
    \end{align}
    %
    Notice that the score is linear in $x$. To make this relationship explicit, we write $S_c(\theta; x) = M(\theta) x$, where $M(\theta) \in \bR^{2 \times 6}$ consists of the coefficients on $x$ in (\ref{eq:comp_score1}) and (\ref{eq:comp_score2}). We will make use of this linearity later.

    Next, we give the information matrix for the complete data.
    %
    \begin{align}
        I_c(\theta;x) &= - \begin{bmatrix}
            \partial^2_p \ell_c(\theta; x) & \partial_{p,q} \ell_c(\theta; x)\\
            \partial_{p,q} \ell_c(\theta; x) & \partial^2_q \ell_c(\theta; x)
        \end{bmatrix} \mathrm{, where}\\
        \partial^2_p \ell_c(\theta; x) &=  \frac{x_2 + 2 x_3 + x_6}{p^2} + \frac{2x_1 + x_2 + x_4}{r^2} = \frac{n_A}{p^2} + \frac{n_O}{r^2}\\
        \partial_{p,q} \ell_c(\theta; x) &=   \frac{2x_1 + x_2 + x_4}{r^2} = \frac{n_O}{r^2}\\
        \partial^2_q \ell_c(\theta; x) &=  \frac{x_4 + 2 x_5 + x_6}{q^2} + \frac{2x_1 + x_2 + x_4}{r^2} = \frac{n_B}{q^2} + \frac{n_O}{r^2}
    \end{align}
    %
    

    \subsection{Missing Data Distribution}
    \label{app:blood_miss}

    Many quantities which arise in the EM and MCEM algorithms depend on the missing data distribution (i.e.\ the conditional distribution of $X$ given $Y=y$). This distribution is best described componentwise in $X$. First, note that $X_1$ and $X_6$ occur in $Y$, so we have $X_1 = y_1$ and $X_6 = y_4$. Next, we have that $X_2 + X_3 = y_2$ and $X_4 + X_5 = y_3$. Thus, we can write $X_2 \sim \mathrm{Bin}(y_2, 2pr / (p^2 + 2pr))$ and $X_4 \sim \mathrm{Bin}(y_3, 2qr / (q^2 + 2qr))$. Finally, we recover $X_3$ and $X_5$ by subtracting $X_2$ from $y_2$ and $X_4$ from $y_3$ respectively.

    We make frequent require the first few conditional moments of $X$. We list them here for convenience. Let $\alpha_1 = 2pr / (p^2 + 2pr)$ be the probability parameter for the binomial distribution of $X_2$ given $Y$, and $\alpha_2 = 1 - \alpha_1$. Similarly, let $\beta_1 = 2qr / (q^2 + 2qr)$ correspond to $X_4$ and $\beta_2 = 1 - \beta_1$. \hl{Notation in this section is new. I'm not sure I will use it.}
    %
    \begin{align}
        \bE(X | Y=y) &= \begin{pmatrix}
            y_1,  y_2 \alpha_1,  y_2 \alpha_2,  y_3 \beta_1,  y_3 \beta_2,  y_4
        \end{pmatrix}^T\\
        &=: \mu_m\\
        \bV(X | Y=y) &= \begin{pmatrix}
            0 & 0 & 0 & 0 & 0 & 0\\
            0 & y_2 \alpha_1 \alpha_2 & - y_2 \alpha_1 \alpha_2 & 0 & 0 & 0\\
            0 & - y_2 \alpha_1 \alpha_2 & y_2 \alpha_1 \alpha_2 & 0 & 0 & 0\\
            0 & 0 & 0 & y_3 \beta_1 \beta_2 & - y_3 \beta_1 \beta_2 & 0\\
            0 & 0 & 0 & -y_3 \beta_1 \beta_2 & y_3 \beta_1 \beta_2 & 0\\
            0 & 0 & 0 & 0 & 0 & 0
        \end{pmatrix}\\
        &=: \Sigma_m\\
        \bE(XX^T | Y=y) &= \Sigma_m + \mu_m \mu_m^T
    \end{align}
    %
    Conditional expectations of the number of alleles of each kind will be of particular interest.
    %
    \begin{align}
        \nu_O & := \bE(n_O|y)\\
         &= 2y_1 + \frac{y_2 pr}{p^2 + 2pr} + \frac{y_3 qr}{q^2 + 2qr}\\
        &= 2y_1 + y_2 \left( \frac{\rho_2}{\rho_2 + \rho_3} \right) + y_3 \left( \frac{\rho_4}{\rho_4 + \rho_5} \right) &\left( = 2y_1 +  y_2 \left( \frac{\rho_2}{\pi_2} \right) + y_3 \left( \frac{\rho_4}{\pi_3} \right) \right)\\
        \nonumber \\
        \nu_A & := \bE(n_A|y)\\
        &= \frac{2 y_2 pr}{p^2 + 2pr} + \frac{2y_2 p^2}{p^2 + 2pr} + y_4\\
        &= y_2 \left( \frac{\rho_2}{\rho_2 + \rho_3} + 2 \frac{\rho_3}{\rho_2 + \rho_3} \right) + y_4 &\left(= y_2 \left( \frac{\rho_2}{\pi_2} + 2\frac{\rho_3}{\pi_2} \right) + y_4 \right)\\
        &= y_2 \left( 1 + \frac{p^2}{p^2 + 2pr} \right) + y_4\\
        \nonumber \\
        \nu_B &:= \bE(n_B|y)\\
        &= \frac{2 y_3 qr}{q^2 + 2qr} + \frac{2y_3 q^2}{q^2 + 2qr} + y_4\\
        &= y_3 \left( \frac{\rho_4}{\rho_4 + \rho_5} + 2\frac{\rho_5}{\rho_4 + \rho_5} \right) + y_4 &\left(= y_3 \left( \frac{\rho_4}{\pi_3} + 2\frac{\rho_5}{\pi_3} \right) + y_4 \right)\\
        &= y_3 \left( 1 + \frac{q^2}{q^2 + 2qr} \right) + y_4
    \end{align}


    \subsection{EM Algorithm}

    In order to apply the EM algorithm, we must construct and optimize the EM objective function. That is, we must compute $Q(\theta|\theta_0) = \bE_{\theta_0} \left[ \ell_c(\theta; y, X) | Y=y \right]$. The conditional distribution of $X$ given $Y=y$ is best described componentwise in $X$. First, note that $X_1$ and $X_6$ occur in $Y$, so we have $X_1 = y_1$ and $X_6 = y_4$. Next, we have that $X_2 + X_3 = y_2$ and $X_4 + X_5 = y_3$. Thus, we can write $X_2 \sim \mathrm{Bin}(y_2, 2pr / (p^2 + 2pr))$ and $X_4 \sim \mathrm{Bin}(y_3, 2qr / (q^2 + 2qr))$. Finally, we recover $X_3$ and $X_5$ by subtracting $X_2$ from $y_2$ and $X_4$ from $y_3$ respectively.
    
    Before writing out the EM objective function, we first compute conditional expectations of the sample allele counts.
    %
    \begin{align}
        \bE(n_O|y) &= 2y_1 + \frac{y_2 pr}{p^2 + 2pr} + \frac{y_3 qr}{q^2 + 2qr}\\
        &= 2y_1 + y_2 \left( \frac{\rho_2}{\rho_2 + \rho_3} \right) + y_3 \left( \frac{\rho_4}{\rho_4 + \rho_5} \right) &\left( = 2y_1 +  y_2 \left( \frac{\rho_2}{\pi_2} \right) + y_3 \left( \frac{\rho_4}{\pi_3} \right) \right)\\
        &=: \nu_O\\
        \nonumber \\
        \bE(n_A|y) &= \frac{2 y_2 pr}{p^2 + 2pr} + \frac{2y_2 p^2}{p^2 + 2pr} + y_4\\
        &= y_2 \left( \frac{\rho_2}{\rho_2 + \rho_3} + 2 \frac{\rho_3}{\rho_2 + \rho_3} \right) + y_4 &\left(= y_2 \left( \frac{\rho_2}{\pi_2} + 2\frac{\rho_3}{\pi_2} \right) + y_4 \right)\\
        &= y_2 \left( 1 + \frac{p^2}{p^2 + 2pr} \right) + y_4\\
        &=: \nu_A\\
        \nonumber \\
        \bE(n_B|y) &= \frac{2 y_3 qr}{q^2 + 2qr} + \frac{2y_3 q^2}{q^2 + 2qr} + y_4\\
        &= y_3 \left( \frac{\rho_4}{\rho_4 + \rho_5} + 2\frac{\rho_5}{\rho_4 + \rho_5} \right) + y_4 &\left(= y_3 \left( \frac{\rho_4}{\pi_3} + 2\frac{\rho_5}{\pi_3} \right) + y_4 \right)\\
        &= y_3 \left( 1 + \frac{q^2}{q^2 + 2qr} \right) + y_4\\
        &=: \nu_B
    \end{align}
    
    
    The EM objective function can be written as
    %
    \begin{align}
		Q(\theta | \theta_0) &:= \bE_{\theta_0} [\ell_c (\theta; X) | Y=y]\\
		&\equiv \nu_O^{(0)} \log r + \nu_A^{(0)} \log p + \nu_B^{(0)} \log q
	\end{align}
    %
    where a superscript zero denotes that the quantity is computed by taking an expectation under $\theta_0$. Maximizing $Q$ analytically with respect to $\theta$ gives the following expression for the EM update:
    %
    \begin{align}
        M(\theta_{k-1}) &= \begin{pmatrix}
            \hat{p}_k\\ \hat{q}_k
        \end{pmatrix}\\
        &= \begin{pmatrix}
            \nu_A^{(k-1)} / 2n\\
            \nu_B^{(k-1)} / 2n
        \end{pmatrix}
    \end{align}
    %
    where a superscript $k-1$ denotes that the quantity is computed by taking an expectation under $\theta_{k-1}$. Note that the equation for a stationary point of the EM algorithm, $M(\theta) = \theta$, has identical roots to the observed data score equation, $S(\theta) = 0$ \hl{(confirm this)}.

    \subsubsection{Asymptotic Standard Error}

    \subsection{Monte Carlo EM}


\end{appendices}

\newpage

\textbf{Check for ``Citation Needed'' before publishing.}

\bibliographystyle{plainnat}
\bibliography{mybib}

\printindex
\end{document}  

