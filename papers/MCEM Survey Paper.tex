\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry} 
\usepackage{color}               		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{setspace}

\usepackage[title]{appendix}   % Start an appendices environment, then treat each separate appendix as an ordinary section
								% [title] changes labels to, e.g., "Appendix A...". Omit to label as "A...".

\usepackage{authblk}

\usepackage{mathrsfs}	% For \mathcal{}, a script font in math mode

\usepackage[semicolon]{natbib}
\usepackage{verbatim}
\usepackage{soul}	% Highlighting
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}
\def\UrlBreaks{\do\/\do-}


\usepackage{multirow}

\usepackage{esdiff} %Shorter syntax for derivatives. Use \diff{f}{x} or \diffp{f}{t}
\usepackage{amsmath}
\usepackage{amsthm}

\usepackage{enumitem}   % For more customizable lists

\usepackage{etoolbox}   % Prerequisite for imakeidx
\usepackage{imakeidx}   % For making an index
\makeindex              % Initialize the index (this command must be included!)


% \usepackage[ruled]{algorithm2e} % For writing algorithms

\usepackage{algpseudocode}
\usepackage{algorithm} % For writing algorithms

\newcommand{\lt}{LTHC}
\newcommand{\llt}{\ell(\theta)}


\newcommand{\bF}{\mathbb{F}}
\newcommand{\bG}{\mathbb{G}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\bQ}{\mathbb{Q}}
\newcommand{\bV}{\mathbb{V}}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\bR}{\mathbb{R}}

\newcommand{\iid}{\overset{\mathrm{iid}}{\sim}}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\cd}{f_c(y, x; \theta)}
\newcommand{\od}{f(y; \theta)}


\newtheorem{proposition}{Proposition}[section]


%SetFonts

%SetFonts


\title{\SARS\ Transmission in University Classes}
\author[1]{William Ruth}
\author[2]{Richard Lockhart}
\affil[1]{Corresponding Author - Department of Statistics and Actuarial Science \\ Simon Fraser University \\ Burnaby, BC  Canada \\ wruth@sfu.ca}
\affil[2]{Department of Statistics and Actuarial Science \\ Simon Fraser University \\ Burnaby, BC  Canada}
%\affil{Department of Statistics and Actuarial Science \\ Simon Fraser University \\ Burnaby, BC  Canada \\ lockhart@sfu.ca}
%\date{\today}							% Activate to display a given date or no date
\date{}

\begin{document}
%\maketitle

% \doublespacing

\begin{abstract}
    We survey the EM algorithm and its Monte Carlo-based extensions.
\end{abstract}

\section{The EM Algorithm}

%todo - Define observed, complete and missing data distributions
%todo - Define parameter space.

The EM algorithm is a method for analyzing incomplete data which was formalized by \citet{Dem77}. See \citet{McL08} for an excellent book-length overview of the EM algorithm. We begin by discussing a probabilistic framework within which the EM algorithm is often applied. We then present the EM algorithm in detail. Finally, we discuss some limitations of this method. Throughout, we illustrate our presentation with a toy problem based on linear regression with a single, unobserved, covariate.

The EM algorithm consists of iterating two steps. First is the expectation, or ``E'', step, in which an objective function is constructed from the complete data likelihood. Second is the maximization, or ``M'', step, in which the previously computed objective function is maximized. These two steps are then alternated until some convergence criterion is met. Whatever value of $\theta$ the algorithm converges to is used as our parameter estimate. We now go into more detail on each of the two steps.

The E-step of the EM algorithm is where we construct the objective function which will be used to update our parameter estimate. This objective function is the conditional expectation of the complete data likelihood, given the observed data. If our complete data can be partitioned into an observed component, $Y$, and a missing component, $X$, then our objective function at iteration $k+1$ is given by
%
\begin{align}
    Q(\theta|\theta_k) & = \bE_{\theta_k}[\ell_c(\theta; y, X) | Y=y]
\end{align}
%
Where $\ell_c$ is the log-likelihood of the complete data model. Note that the conditional expectation uses our parameter estimate from the previous iteration.

The M-step of the EM algorithm consists of maximizing the objective function constructed in the previous E-step. That is, we define $\theta_{k+1} = \argmax\limits_\theta Q(\theta|\theta_k)$. Typically, this optimization must be performed numerically via, e.g., gradient ascent or Newton's method. See \citet{Noc06} for details and other optimization algorithms. 

We can combine the E- and M-steps of the EM algorithm into a single ``update function''. We write $M(\theta_k) = \argmax\limits_\theta Q(\theta|\theta_k)$. The EM algorithm can thus be viewed as the iterative application of this update function, $M$.

\subsection{Properties}

\textbf{Section intro...}

\subsubsection{Ascent Property and Generalized EM}

An important feature of the EM algorithm is its so-called ``ascent property''. This property says that an iteration of the EM algorithm (\hl{have I explicitly defined ``EM iteration''? Do I need to?}) never decreases the observed data likelihood. This is somewhat surprising, since EM updates are computed without ever evaluating the observed data likelihood. 

\begin{proposition}[Ascent Property of EM]
    Let $\theta \in \Theta$, and $\theta' = M(\theta)$ be the EM update from $\theta$. Then $\ell(\theta') \geq \ell(\theta)$.
\end{proposition}

\begin{proof}
    We begin by noting that the following decomposition holds for any value of $x$:
    %
    \begin{align}
        \ell(\theta; y) &= \ell_c(\theta; y, x) - \ell_m(\theta; y, x)
    \end{align}
    %
    Subtracting the values of both sides at $\theta$ from their values at $\theta'$ and taking conditional expectations, we get
    %
    \begin{align}
        \ell(\theta'; y) - \ell(\theta; y) &= Q(\theta'|\theta) - Q(\theta|\theta) + \bE_{\theta}[\ell_m(\theta; y, x) - \ell_m(\theta'; y, x)]\\
        &= Q(\theta'|\theta) - Q(\theta|\theta) + \mathrm{KL}(\theta || \theta') \label{eq:asc_KL}
    \end{align}
    %
    Where the last term in line \ref{eq:asc_KL} is the Kullback-Leibler (KL) divergence from the missing data distribution with $\theta = \theta$ to the same distribution with $\theta = \theta'$. Note that KL divergences are always non-negative, so we get
    %
    \begin{align}
    \ell(\theta'; y) - \ell(\theta; y) &\geq Q(\theta'|\theta) - Q(\theta|\theta)    
    \end{align}
    %
    Finally, since $\theta'$ maximizes $Q(\cdot|\theta)$, we have $\ell(\theta'; y) - \ell(\theta; y) \geq 0$.
\end{proof}

In our proof of the ascent property, we only required that $Q(\theta'|\theta) \geq Q(\theta|\theta)$, not that $\theta'$ maximize $Q(\cdot|\theta)$. This observation leads to the definition of the ``Generalized EM Algorithm''\index{Generalized EM Algorithm}, which replaces the M-step with setting $\theta_{k+1}$ to any point in $\Theta$ such that $Q(\theta_{k+1}|\theta_k) \geq Q(\theta_k|\theta_k)$.



\subsubsection{Recovering Observed Data Likelihood Quantities}

Under regularity conditions, it is possible to compute both the score vector and the observed information matrix of the observed data likelihood using complete data quantities. These regularity conditions consist of being able to interchange the order of differentiation and integration for various functions (\hl{awk? I wasn't feeling well when I wrote this.}). \hl{Define $\mathcal{I}_c$ and $\mathcal{I}_m$.}

\begin{proposition}
    \label{thm:EM_decomp}
    The following identities hold (\hl{regularity conditions!}):
    \begin{enumerate}[label=(\roman*)]
        \item $S(\theta; y) = \bE_\theta [S_c(\theta; y, X)|Y=y]$ \label{eq:obs_score_identity}
        \item $I(\theta) = \mathcal{I}_c(\theta) - \mathcal{I}_m(\theta)$ \label{eq:obs_info_identity}
    \end{enumerate}
\end{proposition}

\begin{proof}
    We start with expression \ref{eq:obs_score_identity}. Let $\Omega$ be the complete data sample space. Let $\mathcal{Y}$ and $\mathcal{X}$ be the observed and missing data sample spaces respectively. For every $y \in \mathcal{Y}$, let $\mathcal{X}(y) = \{ x \in \mathcal{X}: (y,x) \in \Omega\}$. Note that $f(y; \theta) = \int_{\mathcal{X}(y)} f_c(y, x; \theta) dx$.
    %
    \begin{align}
        \bE_\theta [S_c(\theta; y, X)|Y=y] &= \int_{\mathcal{X}(y)} \nabla \ell_c(\theta; y) f_m(y, x; \theta) dx \nonumber\\
        &= \int_{\mathcal{X}(y)} \frac{f_m(y, x; \theta)}{f_c(y, x; \theta)} \nabla f_c(\theta; y) \nonumber\\
        &= \int_{\mathcal{X}(y)} \frac{1}{f(y; \theta)} \nabla f_c(\theta; y)\nonumber\\
        &= \frac{1}{f(y; \theta)} \int_{\mathcal{X}(y)} \nabla f_c(\theta; y)\nonumber\\
        &= \frac{1}{f(y; \theta)} \nabla \int_{\mathcal{X}(y)} f_c(\theta; y) \nonumber\\
        &= \frac{1}{f(y; \theta)} \nabla f(y; \theta)\nonumber\\
        &= S(\theta; y) \nonumber
    \end{align}

    Proceeding now to \ref{eq:obs_info_identity}, we decompose the observed data log-likelihood as
    %
    \begin{align*}
        \ell(\theta; y) &= \ell_c(\theta; y, x) - \ell_m(\theta; y, x)
    \end{align*}
    %
    Differentiating twice and taking conditional expectations of both sides yields the required result.
\end{proof}

An alternative to Proposition \ref{thm:EM_decomp} part \ref{eq:obs_info_identity} which involves only conditional expectations of complete data quantities is given in the following proposition.

\begin{proposition}
    \label{thm:info_decomp}
    Let $\hat{\theta}$ be a stationary point of the observed data log-likelihood. \hl{Assuming regularity conditions,} we can write the observed information of the observed data distribution at $\hat{\theta}$ as
    %
    \begin{align}
        I(\hat{\theta}) = \mathcal{I}_c(\hat{\theta})  - \bE_{\hat{\theta}} [ S_c(\hat{\theta}) S_c(\hat{\theta}) | Y=y]
    \end{align}
\end{proposition}

\begin{proof}
    We follow the derivation of \citet{Lou82}. For brevity, we write $f(\theta)$ and $f_c(\theta)$ for $f(y; \theta)$ and $f(y, x; \theta)$ respectively. Consider the following two Hessians:
    %
    \begin{align}
        \nabla^2 \ell(\theta) &= \nabla \left[ \int_{\mathcal{X}(y)} \frac{\nabla f_c(\theta) dx}{f(\theta)} \right]\\
        &= \int_{\mathcal{X}(y)} \frac{\nabla^2 f_c(\theta)}{f(\theta)} dx - \frac{1}{f(\theta)^2}\left( \int_{\mathcal{X}(y)} \nabla f_c(\theta) dx \right) \left( \int_{\mathcal{X}(y)} \nabla f_c(\theta) dx \right)^T\\
        &= \bE_\theta \left[ \left. \frac{\nabla^2 f_c(\theta)}{f_c(\theta)} \right| Y=y \right] - \bE_\theta \left[ \left. \frac{\nabla f_c(\theta)}{f_c(\theta)} \right| Y=y \right] \bE_\theta \left[ \left. \frac{\nabla f_c(\theta)}{f_c(\theta)} \right| Y=y \right]^T\\
        &= \bE_\theta \left[ \left. \frac{\nabla^2 f_c(\theta)}{f_c(\theta)} \right| Y=y \right] - S(\theta; y) S(\theta; y)^T \label{eq:hess_obs_lik}\\
        \nabla^2 \ell_c(\theta) &= \nabla \left( \frac{\nabla f_c(\theta)}{f_c(\theta)} \right)\\
        &= \frac{\nabla^2 f_c(\theta)}{f_c(\theta)} - S_c(\theta) S_c(\theta)^T \label{eq:hess_comp_lik}
    \end{align}
    %
    Combining lines \ref{eq:hess_obs_lik} and \ref{eq:hess_comp_lik}, we get
    %
    \begin{align}
        \nabla^2 \ell(\theta) &= \bE_\theta [ \nabla^2 \ell_c(\theta) | Y=y] + \bE_\theta [ S_c(\theta) S_c(\theta)^T | Y=y] - S(\theta; y) S(\theta; y)^T \label{eq:hess_obs_lik2}
    \end{align}
    %
    Finally, evaluating this last line at $\theta = \hat{\theta}$ makes the rightmost term vanish, thereby yielding the required expression.
\end{proof}

Proposition \ref{thm:info_decomp} is known as Louis' standard error formula. Other decompositions for the observed information matrix of the observed data likelihood do exist; see, e.g., \citet{Oak99,McL08}. However, the one due to Louis will be most useful to us later.

\subsection{Example: Linear Regression with an Unobserved Covariate}
\label{sec:eg-lin_reg}

Consider the scenario where a measured quantity is known to depend linearly on another unobserved, but nevertheless well understood, quantity. For example, \hl{something, something, census data}. We first present a model for such a scenario, then show how to directly analyze the observed data. Throughout the rest of this document, we will return to this example to illustrate how to perform an analysis when increasing portions of the calculations cannot be performed analytically (\hl{awk}).

Let $X \sim \mathrm{M}(\mu, \tau^2)$, where $\mu \in \bR$ and $\tau > 0$. Let $\varepsilon \sim \mathrm{N}(0, \sigma^2)$ for some $\sigma>0$, and $Y = X \beta + \varepsilon$ where $\beta \in \bR$. We observe an iid sample of $Y$s, but not their corresponding $X$s. We do however, treat $\mu$ and $\tau$ as known. Our goal is to estimate $\beta$ and $\sigma$ from this incomplete data. 


\section{The Monte Carlo EM Algorithm}

The Monte Carlo EM, or MCEM, algorithm was first proposed by \citet{Wei90}, and replaces the conditional expectation in the E-step of the EM algorithm with a Monte Carlo average. That is, at each iteration we generate observations from the conditional distribution of the missing data given the observed data, and average the complete data likelihood over this Monte Carlo sample. This does alleviate the challenge of computing a potentially intractable conditional expectation, but does introduce a number of other difficulties. In the rest of this section, we outline some of these difficulties and methods which have been proposed to address them. We focus particularly on the practical aspects of implementing the MCEM algorithm. For an excellent survey of the theoretical considerations for the MCEM algorithm, see \citet{Nea13}.

\subsection{Quantifying Monte Carlo Uncertainty}

In their seminal work, \citet{Wei90} highlight two important challenges with implementing this method: choosing the Monte Carlo sample size at each iteration, and deciding when to terminate the algorithm. It turns out that solutions to these two problems are often connected via their link to the uncertainty in our Monte Carlo conditional expectations. The recommendations given by \citeauthor{Wei90} on how to solve these problems are mostly informal, but much of the later work on the MCEM algorithm centers around developing more precise solutions \hl{(too strong of a statement?)}.

\citet{Cha95} present an alternative method for choosing the Monte Carlo sample size based on starting with a pilot study and using information near the optimal parameter estimate\footnotemark to choose a Monte Carlo size for the rest of the analysis. In contrast to \citet{Wei90}, \citeauthor{Cha95} use a fixed Monte Carlo size for their analysis.

\footnotetext{\citet{Cha95} present an identity which expresses the observed data log-likelihood ratio as the conditional expectation of the corresponding complete data log-likelihood ratio. Replacing the conditional expectation with a Monte Carlo average gives a natural estimate of the observed data log-likelihood ratio. This approach closely resembles the Monte Carlo Maximum Likelihood method of \citet{Gey94}. See \hl{Section ???}.}

The method of \citet{Boo99} centers on treating each iteration of the MCEM algorithm as an M-estimation problem targeting the deterministic EM update. This framework is quite natural, as an iteration of the MCEM algorithm consists of maximizing a Monte Carlo approximation to the EM objective function. Provided that certain regularity conditions are satisfied (see, e.g., \citealp{van98}), we can estimate the asymptotic standard error of the MCEM update as an estimate of the EM update with the same starting value, with the sampling variability induced by Monte Carlo simulation. \citeauthor{Boo99} then recommend constructing an asymptotic confidence set for the EM update, and increasing the Monte Carlo size if this confidence set contains the previous iteration's parameter estimate\footnotemark.

\footnotetext{For myself: As far as I can tell, in their paper, \citeauthor{Boo99} don't suggest that you check whether augmenting the Monte Carlo size produces a confidence set which does not contain the previous iteration's estimate. I think they're just suggesting you increase M and proceed with the next iteration. This differs from \citet{Caf05} who require that we keep checking and augmenting M until the condition is satisfied. \hl{I don't think this needs to go in the paper, but it's of some interest if you want to implement the method of} \citeauthor{Boo99}}

\newpage

\begin{appendices}
    \section{Likelihood for Linear Regression with an Unobserved Covariate}

    In this appendix, we present details for the analysis of our linear regression example with a single, unobserved, covariate. See Section \ref{sec:eg-lin_reg} for formulation of the model and definition of notation.

    \subsection{Observed Data Likelihood, Score and Information}

    The complete data distribution for our model can be written as follows.
    %
    \begin{align}
        \begin{pmatrix} Y \\ X \end{pmatrix} 
        \sim \mathrm{MVN}\left( \begin{pmatrix}
            \mu \beta\\
            \mu
        \end{pmatrix}, \begin{bmatrix}
            \sigma^2 + \tau^2 \beta^2 & \tau^2 \beta\\
            \tau^2 \beta & \tau^2
        \end{bmatrix} \right) \label{eq:comp_dist}
    \end{align}
    %
    Since our observed data, $Y$, is a marginal of the complete data, we can read off the distribution of $Y$ from Expression (\ref{eq:comp_dist}). That is, $Y \sim \mathrm{N}(\mu \beta, \sigma^2 + \tau^2 \beta^2)$.

    Based on a sample of observed data, $y_1,\ldots, y_n$, our log-likelihood is as follows. Write $\theta = (\beta, \sigma)$ for the vector of unknown parameters, and $\eta^2 = \sigma^2 + \tau^2 \beta^2$ for the marginal variance of $Y$. \hl{In previous work, I used $\eta$ for $\bV Y$. Make sure I adjust any discussion and \textbf{CODE(!!!)} accordingly}.
    %
    \begin{align}
        \ell(\theta; y) &= - \frac{n}{2} \log (2 \pi) - n \log (\eta) - \sum_{i=1}^n \frac{(y_i - \mu \beta)^2}{2 \eta^2}\\
        &\equiv -n \log (\eta) - \sum_{i=1}^n \frac{(y_i - \mu \beta)^2}{2 \eta^2}
    \end{align}
    %
    Where $\equiv$ denotes equality up to additive constants which do not depend on $\theta$.

    The score vector is given by
    %
    \begin{align}
        S(\theta; y) &= \frac{1}{\eta^4}\begin{pmatrix}
            - n \beta^3 \tau^4 - \beta^2 \mu \tau^2 \sum y_i + \beta [\tau^2 \sum y_i^2 - n \sigma^2 (\tau^2 + \sigma^2)] + \mu \sigma^2 \sum y_i\\
            \sigma[n \beta^2 (\mu^2 - \tau^2)  - 2 \beta \mu \sum y_i + \sum y_i^2 - n \sigma^2]
        \end{pmatrix}
    \end{align}

    The observed information matrix is given by
    %
    \begin{align}
        I(\theta; y) &= \frac{1}{\eta^6} \begin{bmatrix}
            I^{(1,1)} & I^{(1,2)}\\
            I^{(1,2)} & I^{(2,2)}
        \end{bmatrix}
    \end{align}
    %
    where
    \begin{align}
        I^{(1,1)} & = - n \beta^4 \tau^6 - 2 \beta^3 \mu \tau^4 \sum y_i + 3 \beta^2 \tau^2 (3 \tau^2 \sum y_i^2 - n \sigma^2 \mu^2) \\
        & + 6 \beta \sigma^2 \tau^2 \mu \sum y_i + \sigma^2 [n \sigma^2 (\tau^2 + \mu^2) - \tau^2 \sum y_i] \nonumber \\
        I^{(1,2)} &= 2 n \beta^3 \sigma \tau^2 (\mu^2 - \tau^2) - 6 \beta^2 \sigma \mu \tau^2 \sum y_i + 2 \beta \sigma [2 \tau^2 \sum y_i^2 - n \sigma^2 (\mu^2 + \tau^2)] \\
        & + 2 \mu \sigma^3 \sum y_i \nonumber \\
        I^{(2,2)} &= n \beta^4 \tau^2 (\tau^2 - \mu^2) + 2 \beta^3 \mu \tau^2 \sum y_i + \beta^2 (3 n \mu^2 \sigma^2 - \tau^2 \sum y_i^2) \\
        & - 6 \beta \mu \tau^2 \sum y_i + \sigma^2 (3 \sum y_i^2 - n \sigma^2) \nonumber
    \end{align}


    As an aside, I did explore the above model with multiple covariates. Unfortunately, marginalizing out $X$ consists of replacing each observed covariate vector with its mean, $\mu$. This results in linearly dependent observations, so the model is overparameterized. I could probably incorporate an intercept term without introducing the overparameterization problem, but I don't think it's worth the effort. I'm not going to be able to sell anyone on the applicability of my model, and adding a third parameter won't really increase the pedagogical value.

\end{appendices}

\textbf{Check for ``Citation Needed'' before publishing.}


\bibliographystyle{plainnat}
\bibliography{mybib}

\printindex
\end{document}  

