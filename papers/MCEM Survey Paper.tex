\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry} 
\usepackage{color}               		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{setspace}

\usepackage[title]{appendix}   % Start an appendices environment, then treat each separate appendix as an ordinary section
								% [title] changes labels to, e.g., "Appendix A...". Omit to label as "A...".

\usepackage{authblk}

\usepackage{mathrsfs}	% For \mathcal{}, a script font in math mode

\usepackage[semicolon]{natbib}
\usepackage{verbatim}
\usepackage{soul}	% Highlighting
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}
\def\UrlBreaks{\do\/\do-}


\usepackage{multirow}

\usepackage{esdiff} %Shorter syntax for derivatives. Use \diff{f}{x} or \diffp{f}{t}
\usepackage{amsmath}
\usepackage{amsthm}

\usepackage{enumitem}   % For more customizable lists

\usepackage{etoolbox}   % Prerequisite for imakeidx
\usepackage{imakeidx}   % For making an index
\makeindex              % Initialize the index (this command must be included!)


% \usepackage[ruled]{algorithm2e} % For writing algorithms

\usepackage{algpseudocode}
\usepackage{algorithm} % For writing algorithms

\usepackage{glossaries}
\makeglossaries



\newcommand{\lt}{LTHC}
\newcommand{\llt}{\ell(\theta)}


\newcommand{\bF}{\mathbb{F}}
\newcommand{\bG}{\mathbb{G}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\bQ}{\mathbb{Q}}
\newcommand{\bV}{\mathbb{V}}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\bR}{\mathbb{R}}

\newcommand{\iid}{\overset{\mathrm{iid}}{\sim}}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\cd}{f_c(y, x; \theta)}
\newcommand{\od}{f(y; \theta)}

\newcommand{\hq}{\hat{Q}}

\newcommand{\wcirc}{\mathring{w}}


\newtheorem{proposition}{Proposition}[section]



\newglossaryentry{densities} %! This doesn't actually work. Inline glossary entries are like citations, they print the name of the entry. I could instead separate each density into its own glossary entry, but this will lead to a large, poorly organized glossary.
{
    name={$f, f_c, f_m$},
    description={Observed data, complete data, missing data densities (or PMFs)}
}


\title{\SARS\ Transmission in University Classes}
\author[1]{William Ruth}
\author[2]{Richard Lockhart}
\affil[1]{Corresponding Author - Department of Statistics and Actuarial Science \\ Simon Fraser University \\ Burnaby, BC  Canada \\ wruth@sfu.ca}
\affil[2]{Department of Statistics and Actuarial Science \\ Simon Fraser University \\ Burnaby, BC  Canada}
%\affil{Department of Statistics and Actuarial Science \\ Simon Fraser University \\ Burnaby, BC  Canada \\ lockhart@sfu.ca}
%\date{\today}							% Activate to display a given date or no date
\date{}

\begin{document}
%\maketitle

% \doublespacing

\begin{abstract}
    We survey the EM algorithm and its Monte Carlo-based extensions.
\end{abstract}

\begin{itemize}
    \color{red}
    \item Replace ``conditional distribution of the missing data given the observed data'' with ``missing data distribution''
    \item \citet{Caf05} use $M_k$ for the Monte Carlo size at iteration $k$. This is a useful definition for discussing other papers too.
    \item I have changed my iteration labels. $\hat{\theta}_{k-1}$ is now the maximizer of the current MCEM objective function, and $\hat{\theta}_{k-1}$ was used to construct that objective function. Watch for this when editing.
    \item Parameter space has not yet been defined.
    \item Is the ``a'' in EM algorithm capitalized or not?
\end{itemize}

\section{The EM Algorithm}

%todo - Define observed, complete and missing data distributions
%todo - Define parameter space.

We define three distributions which will be central to our study of missing data problems. Let $Y$ be the observed data and $X$ be the missing data. Note that $X$ need not correspond to any actual real-world process, but may instead be a conceptual device which facilitates analysis of the data which were actually observed. We refer to the distribution of $Y$ as the ``observed data distribution''\index{Observed Data Distribution}, and write $f$ for its density (or mass function). We refer to the joint distribution of $Y$ and $X$ as the ``complete data distribution''\index{Complete Data Distribution}, and write $f_c$ for its density. We refer to the conditional distribution of $X$ given $Y$ as the ``missing data distribution''\index{Missing Data Distribution}, and write $f_m$ for its density. Note that the missing data distribution is not the marginal distribution of the missing data, but rather its conditional distribution given the observed data.

The EM algorithm is a method for analyzing incomplete data which was formalized by \citet{Dem77}. See \citet{McL08} for an excellent book-length overview of the EM algorithm. We begin by discussing a probabilistic framework within which the EM algorithm is often applied. We then present the EM algorithm in detail. Finally, we discuss some limitations of this method. Throughout, we illustrate our presentation with a toy problem based on linear regression with a single, unobserved, covariate.

The EM algorithm consists of iterating two steps. First is the expectation, or ``E'', step, in which an objective function is constructed from the complete data likelihood. Second is the maximization, or ``M'', step, in which the previously computed objective function is maximized. These two steps are then alternated until some convergence criterion is met. Whatever value of $\theta$ the algorithm converges to is used as our parameter estimate. We now go into more detail on each of the two steps.

The E-step of the EM algorithm is where we construct the objective function which will be used to update our parameter estimate. This objective function is the conditional expectation of the complete data likelihood, given the observed data. If our complete data can be partitioned into an observed component, $Y$, and a missing component, $X$, then our objective function at iteration $k$ is given by
%
\begin{align}
    Q(\theta|\theta_{k-1}) & = \bE_{\theta_{k-1}}[\ell_c(\theta; y, X) | Y=y]
\end{align}
%
where $\ell_c$ is the log-likelihood of the complete data model. Note that the conditional expectation uses our parameter estimate from the previous iteration.

The M-step of the EM algorithm consists of maximizing the objective function constructed in the previous E-step. That is, we define $\theta_k = \argmax\limits_\theta Q(\theta|\theta_{k-1})$. Typically, this optimization must be performed numerically via, e.g., gradient ascent or Newton's method. See \citet{Noc06} for details and other optimization algorithms. 

We can combine the E- and M-steps of the EM algorithm into a single ``update function''. We write $M(\theta_{k-1}) = \argmax\limits_\theta Q(\theta|\theta_{k-1})$. The EM algorithm can thus be viewed as the iterative application of this update function, $M$.

\subsection{Properties}

\textbf{Section intro...}

\subsubsection{Ascent Property and Generalized EM}
\label{sec:GEM}

An important feature of the EM algorithm is its so-called ``ascent property''. This property says that an iteration of the EM algorithm (\hl{have I explicitly defined ``EM iteration''? Do I need to?}) never decreases the observed data likelihood. This is somewhat surprising, since EM updates are computed without ever evaluating the observed data likelihood. 

\begin{proposition}[Ascent Property of EM]
    Let $\theta \in \Theta$, and $\theta' = M(\theta)$ be the EM update from $\theta$. Then $\ell(\theta') \geq \ell(\theta)$.
\end{proposition}

\begin{proof}
    We begin by noting that the following decomposition holds for any value of $x$:
    %
    \begin{align}
        \ell(\theta; y) &= \ell_c(\theta; y, x) - \ell_m(\theta; y, x)
    \end{align}
    %
    Subtracting the values of both sides at $\theta$ from their values at $\theta'$ and taking conditional expectations, we get
    %
    \begin{align}
        \ell(\theta'; y) - \ell(\theta; y) &= Q(\theta'|\theta) - Q(\theta|\theta) + \bE_{\theta}[\ell_m(\theta; y, x) - \ell_m(\theta'; y, x)]\\
        &= Q(\theta'|\theta) - Q(\theta|\theta) + \mathrm{KL}(\theta || \theta') \label{eq:asc_KL}
    \end{align}
    %
    where the last term in line \ref{eq:asc_KL} is the Kullback-Leibler (KL) divergence from the missing data distribution with $\theta = \theta$ to the same distribution with $\theta = \theta'$. Note that KL divergences are always non-negative, so we get
    %
    \begin{align}
    \ell(\theta'; y) - \ell(\theta; y) &\geq Q(\theta'|\theta) - Q(\theta|\theta)    
    \end{align}
    %
    Finally, since $\theta'$ maximizes $Q(\cdot|\theta)$, we have $\ell(\theta'; y) - \ell(\theta; y) \geq 0$.
\end{proof}

In our proof of the ascent property, we only required that $Q(\theta'|\theta) \geq Q(\theta|\theta)$, not that $\theta'$ maximize $Q(\cdot|\theta)$. This observation leads to the definition of the ``Generalized EM Algorithm''\index{Generalized EM Algorithm}, which replaces the M-step with setting $\theta_k$ to any point in $\Theta$ such that $Q(\theta_k|\theta_{k-1}) \geq Q(\theta_{k-1}|\theta_{k-1})$.



\subsubsection{Recovering Observed Data Likelihood Quantities}

Under regularity conditions, it is possible to compute both the score vector and the observed information matrix of the observed data likelihood using complete data quantities. These regularity conditions consist of being able to interchange the order of differentiation and integration for various functions (\hl{awk? I wasn't feeling well when I wrote this.}). \hl{Does it make sense to define $\mathcal{I}_c$ and $\mathcal{I}_m$ in the following proposition?}

\begin{proposition}
    \label{thm:EM_decomp}
    The following identities hold (\hl{regularity conditions!}):
    \begin{enumerate}[label=(\roman*)]
        \item $S(\theta; y) = \bE_\theta [S_c(\theta; y, X)|Y=y]$ \label{eq:obs_score_identity}
        \item $I(\theta) = \mathcal{I}_c(\theta) - \mathcal{I}_m(\theta)$ \label{eq:obs_info_identity}\\
        where $\mathcal{I}_c(\theta) := - \bE_\theta \left[ \nabla^2 \ell_c(\theta; y, X) | Y=y \right]$ and $\mathcal{I}_m(\theta) := - \bE_\theta \left[ \nabla^2 \ell_m(\theta; y, X) | Y=y \right]$
    \end{enumerate}
\end{proposition}

\begin{proof}
    We start with expression \ref{eq:obs_score_identity}. Let $\Omega$ be the complete data sample space. Let $\mathcal{Y}$ and $\mathcal{X}$ be the observed and missing data sample spaces respectively. For every $y \in \mathcal{Y}$, let $\mathcal{X}(y) = \{ x \in \mathcal{X}: (y,x) \in \Omega\}$. Note that $f(y; \theta) = \int_{\mathcal{X}(y)} f_c(y, x; \theta) dx$.
    %
    \begin{align}
        \bE_\theta [S_c(\theta; y, X)|Y=y] &= \int_{\mathcal{X}(y)} \nabla \ell_c(\theta; y, x) f_m(y, x; \theta) dx \nonumber\\
        &= \int_{\mathcal{X}(y)} \frac{f_m(y, x; \theta)}{f_c(y, x; \theta)} \nabla f_c(\theta; y, x) dx \nonumber\\
        &= \int_{\mathcal{X}(y)} \frac{1}{f(y; \theta)} \nabla f_c(\theta; y, x) dx\nonumber\\
        &= \frac{1}{f(y; \theta)} \int_{\mathcal{X}(y)} \nabla f_c(\theta; y, x) dx\nonumber\\
        &= \frac{1}{f(y; \theta)} \nabla \int_{\mathcal{X}(y)} f_c(\theta; y, x) dx \nonumber\\
        &= \frac{1}{f(y; \theta)} \nabla f(y; \theta)\nonumber\\
        &= S(\theta; y) \nonumber
    \end{align}

    Proceeding now to \ref{eq:obs_info_identity}, we decompose the observed data log-likelihood as
    %
    \begin{align*}
        \ell(\theta; y) &= \ell_c(\theta; y, x) - \ell_m(\theta; y, x)
    \end{align*}
    %
    Differentiating twice and taking conditional expectations of both sides yields the required result.
\end{proof}

An alternative to Proposition \ref{thm:EM_decomp} part \ref{eq:obs_info_identity} which involves only conditional expectations of complete data quantities is given in the following proposition.

\begin{proposition}
    \label{thm:info_decomp}
    Let $\hat{\theta}$ be a stationary point of the observed data log-likelihood. \hl{Assuming regularity conditions,} we can write the observed information of the observed data distribution at $\hat{\theta}$ as
    %
    \begin{align}
        I(\theta) = \mathcal{I}_c(\theta) - \bE_{\theta} [ S_c(\theta) S_c(\theta)^T | Y=y] + S(\theta) S(\theta)^T
    \end{align}
    %
    In particular, if $\hat{\theta}$ is a stationary point of the observed data log-likelihood, then
    %
    \begin{align}
        I(\hat{\theta}) = \mathcal{I}_c(\hat{\theta})  - \bE_{\hat{\theta}} [ S_c(\hat{\theta}) S_c(\hat{\theta})^T | Y=y]
    \end{align}
\end{proposition}

\begin{proof}
    We follow the derivation of \citet{Lou82}. For brevity, we write $f(\theta)$ and $f_c(\theta)$ for $f(y; \theta)$ and $f(y, x; \theta)$ respectively. Consider the following two Hessians:
    %
    \begin{align}
        \nabla^2 \ell(\theta) &= \nabla \left[ \int_{\mathcal{X}(y)} \frac{\nabla f_c(\theta) dx}{f(\theta)} \right]\\
        &= \int_{\mathcal{X}(y)} \frac{\nabla^2 f_c(\theta)}{f(\theta)} dx - \frac{1}{f(\theta)^2}\left( \int_{\mathcal{X}(y)} \nabla f_c(\theta) dx \right) \left( \int_{\mathcal{X}(y)} \nabla f_c(\theta) dx \right)^T\\
        &= \bE_\theta \left[ \left. \frac{\nabla^2 f_c(\theta)}{f_c(\theta)} \right| Y=y \right] - \bE_\theta \left[ \left. \frac{\nabla f_c(\theta)}{f_c(\theta)} \right| Y=y \right] \bE_\theta \left[ \left. \frac{\nabla f_c(\theta)}{f_c(\theta)} \right| Y=y \right]^T\\
        &= \bE_\theta \left[ \left. \frac{\nabla^2 f_c(\theta)}{f_c(\theta)} \right| Y=y \right] - S(\theta; y) S(\theta; y)^T \label{eq:hess_obs_lik}\\
        \nabla^2 \ell_c(\theta) &= \nabla \left( \frac{\nabla f_c(\theta)}{f_c(\theta)} \right)\\
        &= \frac{\nabla^2 f_c(\theta)}{f_c(\theta)} - S_c(\theta) S_c(\theta)^T \label{eq:hess_comp_lik}
    \end{align}
    %
    Combining lines \ref{eq:hess_obs_lik} and \ref{eq:hess_comp_lik}, we get
    %
    \begin{align}
        \nabla^2 \ell(\theta) &= \bE_\theta [ \nabla^2 \ell_c(\theta) | Y=y] + \bE_\theta [ S_c(\theta) S_c(\theta)^T | Y=y] - S(\theta; y) S(\theta; y)^T \label{eq:hess_obs_lik2}
    \end{align}
    %
    Finally, evaluating line \ref{eq:hess_obs_lik2} at $\theta = \hat{\theta}$ makes the rightmost term vanish, thereby yielding the required expression.
\end{proof}

Proposition \ref{thm:info_decomp} is known as Louis' standard error formula. Other decompositions for the observed information matrix of the observed data likelihood do exist; see, e.g., \citet{Oak99,McL08}. However, the one due to Louis will be most useful to us later.

\subsection{Example: Linear Regression with an Unobserved Covariate}
\label{sec:eg-lin_reg}

Consider the scenario where a measured quantity is known to depend linearly on another unobserved, but nevertheless well understood, quantity. For example, \hl{something, something, census data}. We first present a model for such a scenario, then show how to directly analyze the observed data. Throughout the rest of this document, we will return to this example to illustrate how to perform an analysis when increasing portions of the calculations cannot be performed analytically (\hl{awk}).

Let $X \sim \mathrm{N}(\mu, \tau^2)$, where $\mu \in \bR$ and $\tau > 0$. Let $\varepsilon \sim \mathrm{N}(0, \sigma^2)$ for some $\sigma>0$, and $Y = X \beta + \varepsilon$ where $\beta \in \bR$. We observe an iid sample of $Y$s, but not their corresponding $X$s. We do however, treat $\mu$ and $\tau$ as known. Our goal is to estimate $\beta$ and $\sigma$ from this incomplete data. 


\section{The Monte Carlo EM Algorithm}

The Monte Carlo EM, or MCEM, algorithm was first proposed by \citet{Wei90}. This method proceeds by replacing the conditional expectation in the E-step of the EM algorithm with a Monte Carlo average. More precisely, at each iteration we generate observations from the conditional distribution of the missing data given the observed data, and average the complete data likelihood over this Monte Carlo sample. Formally, at a given iteration of the MCEM algorithm, let $X_1,\ldots, X_M$ be a Monte Carlo sample from the law of $X|Y=y$ with $\theta$ set to the value from the previous iteration, say $\theta_0$. Write
%
\begin{align}
    \hat{Q}(\theta|\theta_0) &= \sum_{i=1}^M w_i \ell_c(\theta; y, X_i)\\
    &:= \hat{\bE} \ell_c (\theta; y, X)
\end{align}
%
where the $w_i$ are sampling weights. \hl{Confirm that this operator notation isn't contradicted elsewhere.} Under iid sampling we simply get $w_i = M^{-1}$ for every $i$, but more intricate sampling schemes may have more complicated weights. The estimate of $\theta$ is then the maximizer of the MCEM objective function: $\hat{\theta} = \argmax_\theta \hat{Q}(\theta|\theta_0)$. Write $\hat{\theta}_{k-1}$ for the $k$th MCEM estimate. 

Provided that a valid sampling scheme is available for the missing data distribution, we can use Proposition \ref{thm:info_decomp} to estimate the observed data information matrix. 

\begin{proposition}
    Under the conditions of Proposition \ref{thm:info_decomp}, \hl{as well as any required for the sampler,} we get
    %
    \begin{align}
        - \hat{\bE}_{\hat{\theta}} \nabla^2 \ell_c(\hat{\theta}) - \hat{\bE}_{\hat{\theta}} S_c(\hat{\theta}) S_c(\hat{\theta})^T \rightarrow I(\theta)
    \end{align}
    %
    \hl{Under stronger conditions,} we also get asymptotic normality with variance obtained from importance sampling analysis.
\end{proposition}

The MCEM algorithm has the advantage of circumventing the challenge of computing potentially intractable conditional expectations for the EM algorithm. However, this analytical simplification does come at the cost of introducing some new computational problems. In this section, we outline the main problems faced by the MCEM algorithm and present various solutions which have been proposed in the literature. We focus primarily on practical aspects of the MCEM algorithm; see \citet{Nea13} for a survey of theoretical considerations.

Two problems which have received considerable attention in the literature are how to choose the Monte Carlo sample size at each iteration, and how to decide when to terminate the MCEM algorithm. These were identified as early as \citet{Wei90}, but did not receive systematic treatment until later. We here give a brief overview of different authors' approaches to solving these two problems, and spend the rest of this section going into more detail on each method individually. \citet{Wei90} suggest examining a plot of the parameter estimates across iterations, and either terminating or increasing the Monte Carlo size when the plot appears to stabilize. \citet{Cha95} use a pilot study to choose the Monte Carlo sample size, and terminate when a confidence interval for the improvement of the observed data log-likelihood between successive iterations contains zero. \citet{Boo99} frame each MCEM iteration as an M-estimation problem targeting the deterministic EM update. They increase the Monte Carlo size if an asymptotic confidence interval for the EM update contains the previous iteration's parameter estimate, and terminate when multiple successive iterations' estimates have sufficiently small relative error. \citet{Caf05} build confidence bounds for the increment in the EM objective function at each iteration of the MCEM algorithm. They increase the Monte Carlo size until the lower bound is positive and terminate when the upper bound is sufficiently small.

In the rest of this section, we give more detail on each of the implementations introduced above.

\subsection{Early Work (\citealp{Wei90})}

In their seminal work, \citet{Wei90} propose the MCEM algorithm and present a simple implementation. They illustrate that the complete data gradient and Hessian are easily obtained at each iteration from the Monte Carlo sample and, following \citet{Lou82}, give an estimator for the observed data information matrix. Regarding convergence, \citeauthor{Wei90} recommend plotting the parameter estimates across iterations and stopping when the estimates appear to stabilize around some constant. When this stabilization is detected, one can either declare convergence and stop, or increase the Monte Carlo size and continue iterating until the estimate trajectory again stabilizes.

\subsection{Running a Pilot Study}



\subsection{Uncertainty Quantification for the Parameter Estimate (\citealp{Boo99})}

Building on the ideas of \citeauthor{Wei90}, \citet{Boo99} seek to start the MCEM algorithm with a small Monte Carlo size, and add more observations only when the parameter estimates are no longer changing discernibly across iterations. To this end, they recommend building a confidence interval for the EM update based on the Monte Carlo variability of the MCEM update at each iteration. If this interval contains the previous iteration's parameter estimate, then the parameter updates are too small relative to the amount of Monte Carlo variability and more samples are required. Similarly, \citeauthor{Boo99} recommend assessing convergence by checking for small relative error in the parameter updates. To account for the possibility of Monte Carlo variability leading to two consecutive estimates being similar before the algorithm has `converged', the authors suggest waiting until the relative error is small for three consecutive iterations.

The confidence interval used to quantify Monte Carlo uncertainty within an iteration is obtained by framing the parameter update as the solution of an M-estimation problem. This allows us to inherit the desirable properties of M-estimators; specifically, asymptotic normality. See, e.g. \citet{van98}. Following the usual M-estimator construction and assuming that the relevant regularity conditions hold, we are able to estimate the asymptotic variance of the MCEM parameter estimator at each iteration. Note that this standard error is based on the Monte Carlo variability within an iteration; it does not measure sampling variability due to the observed data. 

More formally, write $\tilde{\theta}_k$ for the EM update based on $\hat{\theta}_{k-1}$. Note that $\hat{\theta}_{k-1}$ is held fixed here and in the next subsection. \hl{Analysis of a single MCEM iteration is done conditional on the previous iteration (awk?).} Unless stated otherwise, all expectations are taken with $\theta = \hat{\theta}_{k-1}$. Assuming sufficient smoothness and moment conditions, we get the following expression for the MCEM update:
%
\begin{align}
    \sqrt{M}(\hat{\theta}_k - \tilde{\theta}_k) &= - \sqrt{M} \left[ \nabla^2 Q(\tilde{\theta}_k|\hat{\theta}_{k-1})\right]^{-1} \left[\nabla \hq(\tilde{\theta}_k|\hat{\theta}_{k-1}) \right] + o_p(1) \label{eq:th_as_dist}
\end{align}
%
where $M$ is the Monte Carlo size and $\nabla$ denotes differentiation with respect to the left argument of $Q$ or $\hat{Q}$. Note that the first expression on the right-hand side is the inverse Hessian of the EM objective function (fixed) while the second is the gradient of the MCEM objective function (an average). Thus, $\hat{\theta}_k$ is asymptotically normal with asymptotic variance
%
\begin{align}
    &\left[ \nabla^2 Q(\tilde{\theta}_k|\hat{\theta}_{k-1})\right]^{-1} \bV \left[ S_c (\tilde{\theta}_k) | Y=y \right] \left[ \nabla^2 Q(\tilde{\theta}_k|\hat{\theta}_{k-1})\right]^{-1}\\
    &\approx \left[ \nabla^2 \hat{Q}(\hat{\theta}_k|\hat{\theta}_{k-1})\right]^{-1} \hat{\bE} \left[ S_c(\hat{\theta}_k) S_c(\hat{\theta}_k)^T | Y=y \right] \left[ \nabla^2 \hat{Q}(\hat{\theta}_k|\hat{\theta}_{k-1})\right]^{-1} 
\end{align}
%
where $S_c$ is the complete data score vector, and $\hat{\bE}$ is the Monte Carlo average over the missing data, with $\hat{\theta}_k$ held fixed \hl{(remove comma?)}. Note that there is no first moment term in the conditional variance of $S_c$ because $\hat{\theta}_k$ is a maximizer of $\hat{\bE} [\ell_c |Y=y]$.

Based on the above discussion, we can build an asymptotic confidence interval for $\tilde{\theta}_k$, the EM update based on the MCEM estimate from iteration $k$. \citeauthor{Boo99} recommend checking whether this interval contains $\hat{\theta}_{k-1}$ and, if so, increasing $M$ for the next iteration. Specifically, they suggest starting the next iteration with $M/r$ more points, with $r = 3,4$ or $5$ working well in their examples \hl{(this whole discussion is pretty awkward)}.

To assess convergence of the MCEM algorithm, \citeauthor{Boo99} present two criteria. The first is a familiar measure of relative error in parameter estimates between consecutive iterations:
%
\begin{align}
    \max_j \left( \frac{\left| \hat{\theta}_{k, j} - \hat{\theta}_{k-1,j} \right|}{\left| \hat{\theta}_{k-1,j} \right| + \delta_1} \right) < \delta_2 \label{eq:Boo99_tol}
\end{align}
%
where $\delta_1$ and $\delta_2$ are small positive constants, and the subscript $j$ ranges over components of $\theta$. \citeauthor{Boo99} suggest using $\delta_1 = 10^{-3}$ and $\delta_2$ between $2 \cdot 10^{-3}$ and $5 \cdot 10^{-3}$. See \citep{need} (probably \citealp{Sea06}, p.\ 436, or \citealp{Mar63}) for why condition (\ref{eq:Boo99_tol}) has this particular form.

Alternatively, since \citeauthor{Boo99} apply their method to the analysis of generalized linear mixed models, where pathologies may arise due to parameter estimates being too close to a boundary, they propose a second stopping rule:
%
\begin{align}
    \max_j \left( \frac{\left| \hat{\theta}_{k, j} - \hat{\theta}_{k-1,j} \right|}{\sqrt{\bV \hat{\theta}_{k-1,j}} + \delta'_1} \right) < \delta'_2 \label{eq:Boo99_tol2}
\end{align}

\hl{How do we estimate the variance here?} The purpose of condition (\ref{eq:Boo99_tol2}) is to detect when estimated variance components are very close to zero and the numerical precision needed to satisfy condition (\ref{eq:Boo99_tol}) requires a prohibitive amount of computation.

\subsection{Uncertainty Quantification for the Objective Function \citep{Caf05}}

The approach of \citet{Caf05} is similar in spirit to that of \citet{Boo99}. Both sets of authors seek to quantify Monte Carlo uncertainty in the MCEM algorithm as an approximation to the EM algorithm. The difference is that where \citeauthor{Boo99} measure uncertainty in the parameter estimate, \citeauthor{Caf05} focus on uncertainty in the objective function. Specifically, \citeauthor{Caf05} base their analysis on asymptotic normality of the MCEM increment:
%
\begin{proposition}
    \label{thm:Caf_normality}
    Let $\Delta \hat{Q}(\hat{\theta}_k|\hat{\theta}_{k-1}) = \hat{Q}(\hat{\theta}_{k-1}|\hat{\theta}_{k-1}) - \hat{Q}(\hat{\theta}_k|\hat{\theta}_{k-1})$. Define $\Delta Q(\hat{\theta}_k|\hat{\theta}_{k-1})$ similarly. Let $M_k$ be the Monte Carlo size at iteration $k$. Then, \hl{assuming some regularity conditions},
    %
    \begin{align}
        \sqrt{M_k} \left[ \Delta \hat{Q}(\hat{\theta}_k|\hat{\theta}_{k-1}) - \Delta Q(\hat{\theta}_k|\hat{\theta}_{k-1}) \right] \rightsquigarrow N(0, \Sigma_k)
    \end{align}
    %
    As $M_k \rightarrow \infty$, where $\Sigma_k$ is an asymptotic covariance matrix.
\end{proposition}

\begin{proof}
    Following \citet{Caf05}, we write
    %
    \begin{align}
        \sqrt{M_k} \left[ \Delta \hat{Q}(\hat{\theta}_k|\hat{\theta}_{k-1}) - \Delta Q(\hat{\theta}_k|\hat{\theta}_{k-1}) \right] &= \sqrt{M_k} \left[ \Delta \hat{Q}(\theta_k|\hat{\theta}_{k-1}) - \Delta Q(\theta_k|\hat{\theta}_{k-1}) \right] \nonumber \\
        & + \sqrt{M_k} \left[ \Delta \hat{Q}(\hat{\theta}_k|\hat{\theta}_{k-1}) - \Delta Q(\theta_k|\hat{\theta}_{k-1}) \right]\\
        & + \sqrt{M_k} \left[ \Delta \hat{Q}(\theta_k|\hat{\theta}_{k-1}) - \Delta Q(\hat{\theta}_k|\hat{\theta}_{k-1}) \right]\\
        &=: A_k + B_k + C_k
    \end{align}
    %
    First, note that $A_k$ depends on the current Monte Carlo sample only through $\hat{Q}$, and is thus asymptotically normal by the ordinary Central Limit Theorem. However, $B_k$ and $C_k$ require a more careful analysis.

    \textbf{Use Taylor's Theorem when you're more awake.}
\end{proof}

Provided that we are able to estimate $\Sigma_k$, Proposition \ref{thm:Caf_normality} allows us to build asymptotic confidence intervals for the EM increment, $\Delta Q$. Recall that in Section \ref{sec:GEM}, we defined the Generalized EM algorithm by requiring that $\Delta Q \geq 0$, and showed that this requirement guarantees the ascent property. While the stochastic nature of the MCEM algorithm makes it impossible to guarantee that the EM increment is positive, we are able to use Proposition \ref{thm:Caf_normality} to construct asymptotic confidence bounds for $\Delta Q$. Provided that we can estimate $\Sigma_k$, we can then \hl{be reasonably confident that $\Delta Q > 0$ (awk)}.

Estimating the asymptotic variance under iid or importance sampling is fairly straightforward. Importance sampling however, is somewhat more complicated; particularly when a normalizing constant must be estimated. \citeauthor{Caf05} give a formula for importance sampling based on the Delta Method. They also give some guidance for calculating standard errors based on MCMC sampling, \hl{which we do not go into here. See Section XX for some details}. 

We now return to the key MCEM problems of choosing the Monte Carlo size and when to terminate. For the former, \citeauthor{Caf05} advise constructing a lower confidence limit for the EM increment, $\Delta Q$. If this limit is positive, then we proceed to the next iteration. If not, then we augment the Monte Carlo sample at the current iteration (with, say, $M_k/r$, with $r$ some small positive integer as in \citealp{Boo99}), and compute a new confidence bound. At the next iteration, \citeauthor{Caf05} advise using a starting Monte Carlo sample which is at least as large as the final sample from the previous iteration. \hl{In fact, a larger sample may be required based on extrapolating the MC variability from the previous iteration (clarify; I don't fully understand what they're doing here).}

\citeauthor{Caf05} base their termination criterion on stopping when there is evidence that the algorithm is no longer yielding sufficient improvement in the EM objective function. Specifically, they start by choosing a tolerance, $\tau>0$, then calculate an upper confidence limit for the EM increment at each iteration. If this upper confidence limit is below $\tau$, then we declare that there is little room for improvement left in the EM objective, and terminate our algorithm.

\begin{comment}
\subsection{Quantifying Monte Carlo Uncertainty}

In their seminal work, \citet{Wei90} highlight two important challenges with implementing this method: choosing the Monte Carlo sample size at each iteration, and deciding when to terminate the algorithm. It turns out that solutions to these two problems are often connected via their link to the uncertainty in our Monte Carlo conditional expectations. The recommendations given by \citeauthor{Wei90} on how to solve these problems are mostly informal, but much of the later work on the MCEM algorithm centers around developing more precise solutions \hl{(too strong of a statement?)}.

\citet{Cha95} present an alternative method for choosing the Monte Carlo sample size based on starting with a pilot study and using information near the optimal parameter estimate\footnotemark to choose a Monte Carlo size for the rest of the analysis. In contrast to \citet{Wei90}, \citeauthor{Cha95} use a fixed Monte Carlo size for their analysis.

\footnotetext{\citet{Cha95} present an identity which expresses the observed data log-likelihood ratio as the conditional expectation of the corresponding complete data log-likelihood ratio. Replacing the conditional expectation with a Monte Carlo average gives a natural estimate of the observed data log-likelihood ratio. This approach closely resembles the Monte Carlo Maximum Likelihood method of \citet{Gey94}. See \hl{Section ???}.}

The method of \citet{Boo99} centers on treating each iteration of the MCEM algorithm as an M-estimation problem targeting the deterministic EM update. This framework is quite natural, as an iteration of the MCEM algorithm consists of maximizing a Monte Carlo approximation to the EM objective function. Provided that certain regularity conditions are satisfied (see, e.g., \citealp{van98}), we can estimate the asymptotic standard error of the MCEM update as an estimate of the EM update with the same starting value, with the sampling variability induced by Monte Carlo simulation. \citeauthor{Boo99} then recommend constructing an asymptotic confidence set for the EM update, and increasing the Monte Carlo size if this confidence set contains the previous iteration's parameter estimate\footnotemark.

\footnotetext{For myself: As far as I can tell, in their paper, \citeauthor{Boo99} don't suggest that you check whether augmenting the Monte Carlo size produces a confidence set which does not contain the previous iteration's estimate. I think they're just suggesting you increase M and proceed with the next iteration. This differs from \citet{Caf05} who require that we keep checking and augmenting M until the condition is satisfied. \hl{I don't think this needs to go in the paper, but it's of some interest if you want to implement the method of} \citeauthor{Boo99}}
\end{comment}

\section{Simulation}

A further obstacle to implementing the MCEM algorithm which was not addressed in the previous section, is how to generate the Monte Carlo sample. It is in general a hard problem to simulate from arbitrary conditional distributions. In fact, much of the Bayesian Computation literature centers around this problem (see, e.g., \citealp{Gel13}). In this section, we discuss several methods for simulating the necessary observations at each step of the MCEM algorithm.

\subsection{Importance Sampling}

\hl{The beginning of this section is pretty rambly. It will need to be tightened-up.}

Importance sampling is a general framework for approximating expectations under an intractable distribution. The key idea is to replace the $d \bF$ integral of a function, $\phi$, with the $d \bG$ integral of $\phi \cdot (d \bF / d\bG)$. We call $\bF$ the target distribution and $\bG$ the proposal distribution. For ease of exposition, we assume that both $\bF$ and $\bG$ have densities with respect to a common base measure; call these densities $f$ and $g$ respectively.

The importance sampling literature is vast, and we cannot hope to summarize it here in its entirety. A classic reference on importance sampling and other Monte Carlo methods is the book by \citet{Rob04}; particularly Chapters 3 and 4. Chapter 8 of the book by \citet{Cho20} gives a more current overview of importance sampling, with a focus on its application to Sequential Monte Carlo. \citet{Aga17} give a survey paper level treatment of some more theoretical considerations of importance sampling.

\hl{Re-write intro after writing body}. In this section, we focus on the problem of choosing a proposal distribution, $\bG$. We begin with a more formal description of the importance sampling estimator some theory which helps inform the choice of $\bG$, then give some practical guidance. 

When doing importance sampling, we seek to estimate $\phi = \int h f $, for some test function, $h$. To do so, observe that we can re-write $\phi = \int h (f/g) g$, for any proposal density $g$, such that the integral is finite. Typically, we choose $g$ so that it is easy to sample from; let $X_1,\ldots,X_M$ be such a sample. We estimate $\phi$ by $\hat{\phi} = \hat{\bG} h := (1/M) \sum_{i=1}^M w_i h(X_i)$, where $w_i = f(X_i)/g(X_i)$ is referred to as the importance weight for observation $i$. Note that $\bG \hat{\phi} = \int (f/g) h g = \phi$, so  $\hat{\phi}$ is unbiased. It is similarly straightforward to show that the variance of $\hat{\phi}$ is $M^{-1} \bG w^2 h^2$. 

It is often the case that we only know the target density, $f$, up to a proportionality constant, $\alpha_f$. The same may also be true of the proposal density, $g$, where we may have a desirable proposal distribution in mind but may only be able to compute its density up to a proportionality constant, $\alpha_g$. Write $\tilde{f}$ and $\tilde{g}$ for the un-normalized densities. Paralleling our previous development, we write $\tilde{w} = \tilde{f} / \tilde{g}$ for the ratio of the un-normalized densities. Note that $\tilde{w}$ differs from the true likelihood ratio by a third proportionality constant, $\alpha = \alpha_f / \alpha_g$. In order to estimate $\phi$, we must also estimate the constant $\alpha$. To this end, note the following two identities:
%
\begin{align}
    \bG h \tilde{w} &= \frac{\phi}{\alpha}\\
    \bG \tilde{w} &= \frac{1}{\alpha}
\end{align}
%
This suggests using a ratio estimator for $\phi$:
%
\begin{align}
    \tilde{\phi} &:= \frac{\hat{\bG} h \tilde{w}}{\hat{\bG} \tilde{w}} \label{eq:SNIS}
\end{align}
%
The estimator in (\ref{eq:SNIS}) is referred to as the self-normalized importance sampling (SNIS)\index{Self-Normalized Importance Sampling} estimator of $\tilde{\phi}$. Although the numerator and denominator of (\ref{eq:SNIS}) are perfectly well-behaved objects, the full SNIS estimator is best studied in general terms using asymptotic methods. Specifically, noting that $\tilde{\phi}$ is a ratio of averages, a routine analysis using the CLT, LLN and Slutsky's Theorem \citep[p.91]{Cho20} gives (under the usual regularity conditions\footnotemark):
%
\begin{align}
    \sqrt{M} (\tilde{\phi} - \phi) \rightsquigarrow N(0, \sigma^2_{SNIS})
\end{align}
%
where $\sigma^2_{SNIS} = \bG ((h - \phi)^2 \tilde{w}^2) / (\bG \tilde{w})^2 =\alpha^2 \bG ((h - \phi)^2 \tilde{w}^2)$. It is common to define the normalized weights as $\tilde{w}_i / \hat{\bG} \tilde{w}$\index{Normalized Importance Weights}, which I denote (non-standardly) by $\mathring{w}_i$. Thus, we can define $\tilde{\bG} h := \tilde{\phi} = \sum \mathring{w}_i h(X_i)$.

\footnotetext{\textbf{Finite second moment assumptions à la CLT.}}

One way to assess the performance of an importance sample is by computing the so-called ``effective sample size''\index{Effective Sample Size}. This quantity is defined as $\mathrm{ESS} := (\sum \wcirc_i^{2})^{-1}$.

\subsection{Choosing a Proposal Distribution}

When designing an importance sampling scheme to solve a particular problem, it is important to choose an appropriate proposal distribution. The sense in which a proposal distribution should be appropriate can be understood in a few different ways. If multiple test functions must be integrated using the same proposal distribution, or even the same sample, it makes sense to consider a worst-case error analysis over some class of test functions and to choose a proposal for which the bound is not too large. In contrast, if the goal is to estimate the expectation of a single test function, the proposal can be chosen to better facilitate estimation of the particular integrand. In the former case, we choose our proposal distribution based on its relationship with the target distribution alone, while in the latter case we make our choice based on the target distribution and the test function.

\citet{Aga17} give a very readable overview of some bounds for the worst-case error of an importance sampling scheme. These error bounds are simpler if we restrict attention only to bounded test functions, although similar error bounds also exist for unbounded test functions, provided that certain integrability criteria are met. Focusing on bounded test functions for simplicity, the fundamental quantity for the error bounds in \citeauthor{Aga17} is $\rho := \bG \tilde{w}^2 / (\bG \tilde{w})^2$. Specifically, for test functions bounded in absolute value by 1, the worst-case $\mathcal{L}^2$ error is less than $4 \rho / M$, where $M$ is the size of the importance sample. 



\subsection{Particle-Based Methods}

\subsection{Markov Chain Monte Carlo}

\section{Alternatives to the MCEM Algorithm}

In this section, we outline a few alternatives to the MCEM algorithm for maximizing the likelihood of an incomplete dataset. Examples include the Monte Carlo Maximum Likelihood method of \citet{Gey94}, and Variational Inference \citep{Ble17, Tsi08}.

\subsection{Monte Carlo Maximum Likelihood}



\subsection{Stochastic Approximation}

\subsection{Variational Methods}

\textbf{This section was written by memory (i.e. without looking anything up). The idea is to more efficiently get words on the page, then I can fix things later.}

Variational inference is a set of methods for approximating intractable densities \citep{need}. The literature on variational inference is vast. See \citet{Ble17} for a recent survey paper. \citet[Section 13.7]{Gel13} give a textbook-level overview focusing on applications to Bayesian inference. Numerous authors have discussed the connection between variational inference and the EM algorithm; see, e.g., \citet{Nea98,Tsi08}.

The central idea of variational inference is to frame the target density as the exact solution of a functional optimization problem, where the decision variable typically ranges over densities. The domain of functions is then restricted to some set which is easier to work with. Finally, optimization is performed over this restricted class of functions. The result of this optimization is then our approximation to the target density. \hl{This discussion doesn't make it clear whether our estimator is the optimizer or the optimal objective function.} 

More generally, the optimization problem described above may be just one in a sequence of problems which may form part of an iterative algorithm. Herein lies the connection to the EM algorithm. In the M-step of EM, we compute expectations with respect to a particular conditional distribution. To embed this procedure in the variational inference framework, we define an optimization problem whose solution is the same conditional distribution.

We now give some details. First, let $q$ be some probability density for the missing data, $X$. We can write the observed data log-likelihood as:
%
\begin{align}
    \ell(\theta; y) &= \log f_c(y, x; \theta) - \log f_m(y, x; \theta)\\
    &= \log \left[ \frac{f_c(y, x; \theta)}{q(x)} \right] - \log \left[ \frac{f_m(y, x; \theta)}{q(x)} \right]\\
    &= \bQ \log \left[ \frac{f_c(y, X; \theta)}{q(X)} \right] - \bQ \log \left[ \frac{f_m(y, X; \theta)}{q(X)} \right] \label{eq:variational_expectation}\\
    &=: F(q, \theta) + \mathrm{KL}(q \rightarrow f_m(\theta)) \label{eq:variational_decomp}\\
    & \geq F(q, \theta)
\end{align}
%
where line (\ref{eq:variational_expectation}) holds because the left-hand side does not depend on $x$, and the last line follows from non-negativity of the KL divergence. The first term in line (\ref{eq:variational_decomp}) is called the ``evidence lower-bound'' (ELBO), as well as the ``variational free energy'', depending on the field of application \citep{need}. The latter name comes from an analogue in physics \citep{need}. The former name comes from applications to Bayesian inference, where the observed data log-likelihood can be seen as an evidence if we take $Y$ to be the data and $X$ to be the parameters. 

The EM algorithm can be re-framed as alternately maximizing $F$ with respect to $q$ and $\theta$. To see why, first recall that the KL divergence between two distributions is non-negative, and is zero if and only if the two distributions are equal. Starting at a fixed parameter value, $\theta_0$, maximizing $F$ with respect to $q$ is accomplished by setting $q(x) = f_m(y, x; \theta_0)$, as this minimizes the KL divergence in (\ref{eq:variational_decomp}) and the left-hand side is constant in $q$. The resulting ELBO is equal to $Q(\theta|\theta_0) + \xi$, where $Q$ is the EM objective function and $\xi$ is constant in $\theta$. Maximizing the ELBO in $\theta$ is therefore equivalent to maximizing the EM objective function. This maximization gives a new parameter value, which is used as input to the next iteration. See Theorem 1 of \citet{Nea98} for a more formal proof.

While the EM algorithm is obtained by maximizing $q$ over an unrestricted class of densities, other procedures can be formulated by restricting this class. One popular example is the ``mean-field'' approximation, which involves optimizing over the class of densities which factor over their arguments. That is, the class of functions $\mathcal{Q}_{MF}:=\{ \mathrm{Densities\ } q: q(X_1, \ldots, X_p) = \prod_{j=1}^p q_j(X_j) \}$. 

A major advantage of the mean-field approximation is that an iterative algorithm exists for finding the density, $q$, which maximizes the ELBO. This algorithm performs coordinate ascent, and the coordinate updates are closely related to computation of the full conditional distributions in Gibbs sampling \citep{need}. Write $q^{(k)} = \prod q_j^{(k)}$ for the current value of $q$, and $\bQ^{(k)}_{-j}$ for expectation with respect to all the missing variables except $j$, with distributions from $q^{(k)}$ (\hl{awk?}). The update formula is
%
\begin{align}
    q_j^{(k+1)} &\propto \exp \left[ \bQ^{(k)}_{-j} \ell_c(y, x_j, X_{-j}) \right] \label{eq:var_inf_update}
\end{align}
%
where $X_{-j}$ is all the missing variables other than $X_j$. See Section 2.4 of \citet{Ble17} for a derivation of (\ref{eq:var_inf_update}). The overall algorithm consists of repeatedly cycling through updating each coordinate's distribution until some convergence criterion is met (\hl{how is convergence assessed?}).

Note that, so far, our discussion of how to compute the mean-field approximate density for $X$ has not addressed $\theta$. To apply mean-field variational inference to EM-type problems, we substitute the mean-field density into the ELBO and maximize over $\theta$. This new value of $\theta$ is fed back into ($\ref{eq:var_inf_update}$), giving us a different complete data likelihood function and, hence, a new optimal density.


\newpage

\begin{appendices}
    \section{Likelihood for Linear Regression with an Unobserved Covariate}

    In this appendix, we present details for the analysis of our linear regression example with a single, unobserved, covariate. See Section \ref{sec:eg-lin_reg} for formulation of the model and definition of notation.

    \subsection{Observed Data Likelihood, Score and Information}

    The complete data distribution for our model can be written as follows.
    %
    \begin{align}
        \begin{pmatrix} Y \\ X \end{pmatrix} 
        \sim \mathrm{MVN}\left( \begin{pmatrix}
            \mu \beta\\
            \mu
        \end{pmatrix}, \begin{bmatrix}
            \sigma^2 + \tau^2 \beta^2 & \tau^2 \beta\\
            \tau^2 \beta & \tau^2
        \end{bmatrix} \right) \label{eq:comp_dist}
    \end{align}
    %
    Since our observed data, $Y$, is a marginal of the complete data, we can read off the distribution of $Y$ from Expression (\ref{eq:comp_dist}). That is, $Y \sim \mathrm{N}(\mu \beta, \sigma^2 + \tau^2 \beta^2)$.

    Based on a sample of observed data, $y_1,\ldots, y_n$, our log-likelihood is as follows. Write $\theta = (\beta, \sigma)$ for the vector of unknown parameters, and $\eta^2 = \sigma^2 + \tau^2 \beta^2$ for the marginal variance of $Y$. \hl{In previous work, I used $\eta$ for $\bV Y$. Make sure I adjust any discussion and \textbf{CODE(!!!)} accordingly}.
    %
    \begin{align}
        \ell(\theta; y) &= - \frac{n}{2} \log (2 \pi) - n \log (\eta) - \sum_{i=1}^n \frac{(y_i - \mu \beta)^2}{2 \eta^2}\\
        &\equiv -n \log (\eta) - \sum_{i=1}^n \frac{(y_i - \mu \beta)^2}{2 \eta^2}
    \end{align}
    %
    where $\equiv$ denotes equality up to additive constants which do not depend on $\theta$.

    The score vector is given by
    %
    \begin{align}
        S(\theta; y) &= \frac{1}{\eta^4}\begin{pmatrix}
            - n \beta^3 \tau^4 - \beta^2 \mu \tau^2 \sum y_i + \beta [\tau^2 \sum y_i^2 - n \sigma^2 (\tau^2 + \sigma^2)] + \mu \sigma^2 \sum y_i\\
            \sigma[n \beta^2 (\mu^2 - \tau^2)  - 2 \beta \mu \sum y_i + \sum y_i^2 - n \sigma^2]
        \end{pmatrix}
    \end{align}

    The information matrix is given by
    %
    \begin{align}
        I(\theta; y) &= \frac{1}{\eta^6} \begin{bmatrix}
            I^{(1,1)} & I^{(1,2)}\\
            I^{(1,2)} & I^{(2,2)}
        \end{bmatrix}
    \end{align}
    %
    where
    \begin{align}
        I^{(1,1)} & = - n \beta^4 \tau^6 - 2 \beta^3 \mu \tau^4 \sum y_i + 3 \beta^2 \tau^2 (3 \tau^2 \sum y_i^2 - n \sigma^2 \mu^2) \\
        & + 6 \beta \sigma^2 \tau^2 \mu \sum y_i + \sigma^2 [n \sigma^2 (\tau^2 + \mu^2) - \tau^2 \sum y_i] \nonumber \\
        I^{(1,2)} &= 2 n \beta^3 \sigma \tau^2 (\mu^2 - \tau^2) - 6 \beta^2 \sigma \mu \tau^2 \sum y_i + 2 \beta \sigma [2 \tau^2 \sum y_i^2 - n \sigma^2 (\mu^2 + \tau^2)] \\
        & + 2 \mu \sigma^3 \sum y_i \nonumber \\
        I^{(2,2)} &= n \beta^4 \tau^2 (\tau^2 - \mu^2) + 2 \beta^3 \mu \tau^2 \sum y_i + \beta^2 (3 n \mu^2 \sigma^2 - \tau^2 \sum y_i^2) \\
        & - 6 \beta \mu \tau^2 \sum y_i + \sigma^2 (3 \sum y_i^2 - n \sigma^2) \nonumber
    \end{align}


    As an aside, I did explore the above model with multiple covariates. Unfortunately, marginalizing out $X$ consists of replacing each observed covariate vector with its mean, $\mu$. This results in linearly dependent observations, so the model is overparameterized. I could probably incorporate an intercept term without introducing the overparameterization problem, but I don't think it's worth the effort. I'm not going to be able to sell anyone on the applicability of my model, and adding a third parameter won't really increase the pedagogical value.

    \subsection{EM Algorithm}

    In order to apply the EM algorithm, we must construct and optimize the EM objective function. That is, we must compute $Q(\theta|\theta_0) = \bE_{\theta_0} \left[ \ell_c(\theta; y, X) | Y=y \right]$. The conditional distribution of $X$ given $Y=y$ is $N(\mu_y, \tau^2_y)$, with $\mu_y$ and $\tau^2_y$ given by the following expressions:
    %
	\begin{align}
		\mu_y & := \bE (X | Y=y)\\
		&= \mu + \frac{\tau^2 \beta}{\eta} (y - \mu \beta)\\
		&= \rho^2 \frac{y}{\beta} + (1-\rho^2) \mu\\
		\tau^2_y &:= \bV (X | Y=y)\\
		&= \tau^2 - \frac{\tau^4 \beta^2}{\eta}\\
		&= \tau^2 (1 - \rho^2)
	\end{align}
	%
    where $\rho^2 := \tau^2 \beta^2 / \eta$ is the coefficient of determination (i.e.\ $R^2$) between $Y$ and $X$. For notational convenience, we also define $\zeta_y := \bE(X^2 | Y=y) = \tau_y^2 + \mu_y^2$. The complete data log-likelihood is given by
    %
    \begin{align}
        \ell_c(\theta; y, x) &= \left[ - \frac{n}{2} \log 2 \pi - n \log \tau - \frac{1}{\tau^2} \sum (x_i - \mu)^2 \right] \nonumber\\
        & + \left[ - \frac{n}{2} \log 2\pi - n \log \sigma - \frac{1}{\sigma^2} \sum (y_i - x_i \beta)^2 \right] \label{eq:complete_likelihood}\\
        &\equiv -n \log \sigma - \frac{1}{\sigma^2} \sum (y_i - x_i \beta)^2
    \end{align}
    %
    Note that the first term in line (\ref{eq:complete_likelihood}) is the marginal likelihood of $x$, and the second term is the conditional likelihood of $y$ given $x$.
    
    The EM objective function can be written as
    %
    \begin{align}
		Q(\theta | \theta_0) &:= \bE_{\theta_0} [\ell_c (\theta; y, X) | Y=y]\\
		&\equiv \bE_{\theta_0} \left[ \left. -n \log \sigma - \frac{1}{2\sigma^2} \sum (y_i - X_i \beta)^2 \right| Y=y \right]\\
        &= -n \log \sigma - \frac{1}{2\sigma^2} \sum \left( y_i^2 - 2 \beta y_i \mu_{y_i}^{(0)} + \beta^2 \zeta^{(0)}_{y_i} \right)
	\end{align}
    %
    where a superscript zero denotes that the quantity is computed by taking an expectation under $\theta_0$. Maximizing $Q$ analytically with respect to $\theta$ gives the following expression for the EM update:
    %
    \begin{align}
        M(\theta_{k-1}) &= \begin{pmatrix}
            \hat{\beta}_k\\ \hat{\sigma}_k
        \end{pmatrix}\\
        &= \begin{pmatrix}
            {\sum y_i \mu_{y_i}^{(k-1)}} / {\sum \zeta_{y_i}^{(k-1)}}\\
            \frac{1}{n} \bE_{\theta_{k-1}} \left[ \left. \sum (y_i - x_i \hat{\beta}_k)^2 \right| Y=y \right]
        \end{pmatrix}\\
        &= \begin{pmatrix}
            {\sum y_i \mu_{y_i}^{(k-1)}} / {\sum \zeta_{y_i}^{(k-1)}}\\
            \frac{1}{n} \sum \left( y_i^2 - 2 y_i \mu_{y_i}^{(k-1)} \hat{\beta}_k  + \zeta^{(k-1)}_{y_i} \hat{\beta}_k^2  \right)
        \end{pmatrix}
    \end{align}
    %
    where a superscript $k-1$ denotes that the quantity is computed by taking an expectation under $\theta_{k-1}$. Note that the equation for a stationary point of the EM algorithm, $M(\theta) = \theta$, has identical roots to the observed data score equation, $S(\theta) = 0$ \hl{(confirm this)}.

\end{appendices}

\newpage

\textbf{Check for ``Citation Needed'' before publishing.}

\bibliographystyle{plainnat}
\bibliography{mybib}

\printindex
\end{document}  

